{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steliosg23/TextAnalytics-DS-2025/blob/main/TA_Assignment_1_N_grams_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3"
      ],
      "metadata": {
        "id": "dxxNYY4nYbGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(i) Implement a bigram and a trigram language model for sentences, using Laplace smoothing or optionally Kneser- Ney smoothing.\n",
        "\n",
        "In practice, n-gram language models\n",
        "compute the sum of the logarithms of the n-gram probabilities of each sequence, instead of\n",
        "their product and you should do the same.\n",
        "\n",
        "Assume that each sentence starts with the\n",
        "pseudo-token *start* (or two pseudo-tokens *start1*, *start2* for the trigram model) and\n",
        "ends with the pseudo-token *end*.\n",
        "\n",
        "Train your models on a training subset of a corpus. Include in the vocabulary\n",
        "only words that occur, e.g., at least 10 times in the training subset. Use the same vocabulary\n",
        "in the bigram and trigram models. Replace all out-of-vocabulary (OOV) words (in the\n",
        "training, development, test subsets) by a special token *UNK*.\n",
        "\n",
        "Alternatively, use BPEs instead of words (obtaining the BPE vocabulary from your training subset) to\n",
        "avoid unknown words."
      ],
      "metadata": {
        "id": "67WIft8qYgIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U nltk"
      ],
      "metadata": {
        "id": "uSB6cg5JDprR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8579cbb8-1877-4244-b3a0-8fb378e0ce92"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cBeJGofPHRew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b32966df-c2a7-44c8-ecd5-687bb5c6d877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the Reuters corpus from NLTK library."
      ],
      "metadata": {
        "id": "ynjXqq3PKaNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('reuters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8S-A249MDcl",
        "outputId": "1ae364b0-3c58-4dad-9c36-c0dbd744bf4b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the corpus"
      ],
      "metadata": {
        "id": "4JdBbJacKngy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "stelios\n"
      ],
      "metadata": {
        "id": "Roy0Z5fk1-gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import unicodedata\n",
        "import string\n",
        "from collections import Counter\n",
        "# numpy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "DA2r60PL2QLs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Processing the Reuters Corpus\n",
        "\n",
        "### 1. Importing the Reuters Corpus\n",
        "We begin by importing the Reuters corpus from the `nltk` library:\n",
        "\n"
      ],
      "metadata": {
        "id": "9gNaYPUv_Hzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import reuters\n",
        "\n",
        "# Load the Reuters corpus\n",
        "reuters_corpus = reuters.fileids()\n",
        "\n",
        "# Print the first 10 files\n",
        "print(reuters_corpus[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoTeMTMt5IMD",
        "outputId": "c34e7801-4056-4b2b-89d4-3353e516a746"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['test/14826', 'test/14828', 'test/14829', 'test/14832', 'test/14833', 'test/14839', 'test/14840', 'test/14841', 'test/14842', 'test/14843']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Constructing the Final Corpus\n",
        "We then iterate through each file in the corpus, convert its text to lowercase, and append it to a string that will contain the entire corpus."
      ],
      "metadata": {
        "id": "vjHv4iqF_SpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty string to store the final corpus\n",
        "final_corpus = ''\n",
        "\n",
        "# Loop through each file in the Reuters corpus\n",
        "for corpus in reuters_corpus:\n",
        "    # Load the raw text from the file\n",
        "    text = reuters.raw(fileids=corpus)\n",
        "\n",
        "    # Convert the text to lowercase\n",
        "    lower_text = text.lower()\n",
        "\n",
        "    # Add the lowercase text to the final corpus\n",
        "    final_corpus += lower_text\n",
        "\n"
      ],
      "metadata": {
        "id": "2CWoIe8K1sO5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Counting the Number of Words\n",
        "Finally, we calculate the total number of words in the final corpus by splitting the text into words and counting them."
      ],
      "metadata": {
        "id": "Fz4Llcx8_hCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the total number of words in the final corpus\n",
        "print(len(final_corpus.split()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRKenpCB_itu",
        "outputId": "f0e7e0f0-856d-4c1e-ef10-ebe9ffb96390"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1378305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Cleaning Function\n",
        "\n",
        "The `text_cleaning` function is designed to clean the input text by removing unwanted characters, keeping only words and sentence-ending punctuation marks (periods, question marks, exclamation points, and apostrophes). It also removes multiple spaces and newline characters."
      ],
      "metadata": {
        "id": "OiPgi8Ni2INy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def text_cleaning(text):\n",
        "\n",
        "    # Remove all characters except letters (both uppercase and lowercase),\n",
        "    # sentence-ending characters (periods, question marks, exclamation marks),\n",
        "    # and apostrophes. Replace unwanted characters with a space.\n",
        "    corpus = re.sub(r'[^a-zA-Z.?!\\']', ' ', text)\n",
        "\n",
        "    # Remove all left square brackets '[' from the text\n",
        "    corpus = corpus.replace('[', '')\n",
        "\n",
        "    # Replace all right square brackets ']' with a period '.'\n",
        "    corpus = corpus.replace(']', '.')\n",
        "\n",
        "    # Remove specific special characters (like $, @, ^, &, *, (, ), €, :, etc.)\n",
        "    # by replacing them with a space.\n",
        "    corpus = re.sub(r'[[]/$@^&*()€:΄]', ' ', corpus)\n",
        "\n",
        "    # Replace multiple consecutive spaces with a single space\n",
        "    corpus = re.sub(' +', ' ', corpus)\n",
        "\n",
        "    # Replace newline characters '\\n' with a space to maintain continuous text\n",
        "    corpus = corpus.replace('\\n', ' ')\n",
        "\n",
        "    # Return the cleaned text\n",
        "    return corpus\n"
      ],
      "metadata": {
        "id": "Mv1MESLf2Acy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 150 characters of the corpus before cleaning\n",
        "print(f'Before Cleaning: {final_corpus[:150]}...\\n')\n",
        "\n",
        "# Clean the corpus\n",
        "final_corpus = text_cleaning(final_corpus)\n",
        "\n",
        "# Print the first 150 characters of the corpus after cleaning\n",
        "print(f'After Cleaning: {final_corpus[:150]}...')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_k7csJd2Xu4",
        "outputId": "c95730eb-9c72-4014-b899-0e0d116ef4d4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Cleaning: asian exporters fear damage from u.s.-japan rift\n",
            "  mounting trade friction between the\n",
            "  u.s. and japan has raised fears among many of asia's exportin...\n",
            "\n",
            "After Cleaning: asian exporters fear damage from u.s. japan rift mounting trade friction between the u.s. and japan has raised fears among many of asia's exporting na...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-49a543b9bdc1>:18: FutureWarning: Possible nested set at position 1\n",
            "  corpus = re.sub(r'[[]/$@^&*()€:΄]', ' ', corpus)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "def sentence_tokenization(text):\n",
        "    try:\n",
        "        # Tokenizes the input text into a list of sentences using NLTK's sent_tokenize\n",
        "        return nltk.sent_tokenize(text)\n",
        "    except Exception as e:\n",
        "        # Handle any errors that occur during sentence tokenization\n",
        "        print(f\"Error in tokenizing text: {e}\")\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "-Ucm_F0P3Ets"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_list = sentence_tokenization(final_corpus)\n",
        "\n",
        "# Print the first three sentences of the corpus\n",
        "for i in range(10):\n",
        "    print(f\"Sentence {i+1}: {sentence_list[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLE0FJ553H_R",
        "outputId": "6a081d75-14bf-462e-d182-d70d77dafcf0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: asian exporters fear damage from u.s. japan rift mounting trade friction between the u.s. and japan has raised fears among many of asia's exporting nations that the row could inflict far reaching economic damage businessmen and officials said.\n",
            "Sentence 2: they told reuter correspondents in asian capitals a u.s. move against japan might boost protectionist sentiment in the u.s. and lead to curbs on american imports of their products.\n",
            "Sentence 3: but some exporters said that while the conflict would hurt them in the long run in the short term tokyo's loss might be their gain.\n",
            "Sentence 4: the u.s. has said it will impose mln dlrs of tariffs on imports of japanese electronics goods on april in retaliation for japan's alleged failure to stick to a pact not to sell semiconductors on world markets at below cost.\n",
            "Sentence 5: unofficial japanese estimates put the impact of the tariffs at billion dlrs and spokesmen for major electronics firms said they would virtually halt exports of products hit by the new taxes.\n",
            "Sentence 6: we wouldn't be able to do business said a spokesman for leading japanese electronics firm matsushita electric industrial co ltd lt mc.t .\n",
            "Sentence 7: if the tariffs remain in place for any length of time beyond a few months it will mean the complete erosion of exports of goods subject to tariffs to the u.s. said tom murtha a stock analyst at the tokyo office of broker lt james capel and co .\n",
            "Sentence 8: in taiwan businessmen and officials are also worried.\n",
            "Sentence 9: we are aware of the seriousness of the u.s. threat against japan because it serves as a warning to us said a senior taiwanese trade official who asked not to be named.\n",
            "Sentence 10: taiwan had a trade trade surplus of .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Tokenization Function\n",
        "\n",
        "The `word_tokenization` function takes an input text and tokenizes it into a list of words using the NLTK library's `word_tokenize` function. It also handles any exceptions that may occur during the tokenization process."
      ],
      "metadata": {
        "id": "uNJLOfcdCo9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "def word_tokenization(text):\n",
        "    try:\n",
        "        # Tokenizes the input text into a list of words using NLTK's word_tokenize\n",
        "        return nltk.word_tokenize(text)\n",
        "    except Exception as e:\n",
        "        # Handle any errors that occur during word tokenization\n",
        "        print(f\"Error in tokenizing text: {e}\")\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "9vpHoyD0CY0M"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply word_tokenization for each sentence in the sentence list\n",
        "words_in_sentences = [word_tokenization(sentence) for sentence in sentence_list]\n",
        "\n",
        "# Print words for the first three sentences in the corpus\n",
        "for i in range(10):\n",
        "    print(f\"Sentence {i+1}: {words_in_sentences[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-0JT0c1CYxo",
        "outputId": "078267c4-0298-4194-b3ac-082a825e1491"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: ['asian', 'exporters', 'fear', 'damage', 'from', 'u.s.', 'japan', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'u.s.', 'and', 'japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'asia', \"'s\", 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', 'reaching', 'economic', 'damage', 'businessmen', 'and', 'officials', 'said', '.']\n",
            "Sentence 2: ['they', 'told', 'reuter', 'correspondents', 'in', 'asian', 'capitals', 'a', 'u.s.', 'move', 'against', 'japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'u.s.', 'and', 'lead', 'to', 'curbs', 'on', 'american', 'imports', 'of', 'their', 'products', '.']\n",
            "Sentence 3: ['but', 'some', 'exporters', 'said', 'that', 'while', 'the', 'conflict', 'would', 'hurt', 'them', 'in', 'the', 'long', 'run', 'in', 'the', 'short', 'term', 'tokyo', \"'s\", 'loss', 'might', 'be', 'their', 'gain', '.']\n",
            "Sentence 4: ['the', 'u.s.', 'has', 'said', 'it', 'will', 'impose', 'mln', 'dlrs', 'of', 'tariffs', 'on', 'imports', 'of', 'japanese', 'electronics', 'goods', 'on', 'april', 'in', 'retaliation', 'for', 'japan', \"'s\", 'alleged', 'failure', 'to', 'stick', 'to', 'a', 'pact', 'not', 'to', 'sell', 'semiconductors', 'on', 'world', 'markets', 'at', 'below', 'cost', '.']\n",
            "Sentence 5: ['unofficial', 'japanese', 'estimates', 'put', 'the', 'impact', 'of', 'the', 'tariffs', 'at', 'billion', 'dlrs', 'and', 'spokesmen', 'for', 'major', 'electronics', 'firms', 'said', 'they', 'would', 'virtually', 'halt', 'exports', 'of', 'products', 'hit', 'by', 'the', 'new', 'taxes', '.']\n",
            "Sentence 6: ['we', 'would', \"n't\", 'be', 'able', 'to', 'do', 'business', 'said', 'a', 'spokesman', 'for', 'leading', 'japanese', 'electronics', 'firm', 'matsushita', 'electric', 'industrial', 'co', 'ltd', 'lt', 'mc.t', '.']\n",
            "Sentence 7: ['if', 'the', 'tariffs', 'remain', 'in', 'place', 'for', 'any', 'length', 'of', 'time', 'beyond', 'a', 'few', 'months', 'it', 'will', 'mean', 'the', 'complete', 'erosion', 'of', 'exports', 'of', 'goods', 'subject', 'to', 'tariffs', 'to', 'the', 'u.s.', 'said', 'tom', 'murtha', 'a', 'stock', 'analyst', 'at', 'the', 'tokyo', 'office', 'of', 'broker', 'lt', 'james', 'capel', 'and', 'co', '.']\n",
            "Sentence 8: ['in', 'taiwan', 'businessmen', 'and', 'officials', 'are', 'also', 'worried', '.']\n",
            "Sentence 9: ['we', 'are', 'aware', 'of', 'the', 'seriousness', 'of', 'the', 'u.s.', 'threat', 'against', 'japan', 'because', 'it', 'serves', 'as', 'a', 'warning', 'to', 'us', 'said', 'a', 'senior', 'taiwanese', 'trade', 'official', 'who', 'asked', 'not', 'to', 'be', 'named', '.']\n",
            "Sentence 10: ['taiwan', 'had', 'a', 'trade', 'trade', 'surplus', 'of', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Splitting Process\n",
        "\n",
        "In this process, the corpus is split into three subsets: `train_corpus`, `dev_corpus`, and `test_corpus`. This is a common practice in machine learning for training, development, and testing models.\n",
        "\n",
        "### Steps:\n",
        "1. **Set Random Seed for Reproducibility:**\n",
        "   The random seed is set to ensure that the splitting process is reproducible. This means that every time the code runs, the same splits will occur, providing consistency in results.\n",
        "\n",
        "2. **Initial Split into Train and Temporary Corpus:**\n",
        "   The corpus is first split into two sets: the `train_corpus` and a `temp_corpus` which contains 40% of the data. The remaining 60% is used for training. This ensures that a portion of the data is available for training the model.\n",
        "\n",
        "3. **Further Split the Temporary Corpus into Dev and Test Sets:**\n",
        "   The `temp_corpus` (which is 40% of the original corpus) is further split evenly into two parts: the `dev_corpus` (development set) and the `test_corpus` (test set). Each of these subsets will contain 20% of the original corpus. The dev set is used for model tuning, and the test set is kept for evaluating the model's final performance.\n",
        "\n",
        "4. **Resulting Subsets:**\n",
        "   After the splits, you have:\n",
        "   - `train_corpus` containing 60% of the data used for training.\n",
        "   - `dev_corpus` containing 20% of the data used for validation and hyperparameter tuning.\n",
        "   - `test_corpus` containing 20% of the data used for final testing.\n",
        "\n",
        "This splitting process helps in ensuring that the model is trained, validated, and tested on separate data sets to avoid overfitting and ensure robust evaluation.\n"
      ],
      "metadata": {
        "id": "US0LBv6VETXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(2025)\n",
        "\n",
        "# Shuffle and split the corpus into train, dev, and test sets\n",
        "train_corpus, temp_corpus = train_test_split(words_in_sentences, test_size=0.4, random_state=2025)\n",
        "\n",
        "# Further split temp_corpus into dev and test sets\n",
        "dev_corpus, test_corpus = train_test_split(temp_corpus, test_size=0.5, random_state=2025)\n",
        "\n",
        "# Now train_corpus, dev_corpus, and test_corpus are split\n"
      ],
      "metadata": {
        "id": "mf9M8a3xEqp4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a sample of each set\n",
        "print(\"Sample from Training Set:\")\n",
        "print(train_corpus[:3])  # Printing the first 3 samples from the training set\n",
        "\n",
        "print(\"\\nSample from Development Set:\")\n",
        "print(dev_corpus[:3])  # Printing the first 3 samples from the development set\n",
        "\n",
        "print(\"\\nSample from Test Set:\")\n",
        "print(test_corpus[:3])  # Printing the first 3 samples from the test set\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1k3j5XRFAzf",
        "outputId": "4bdd92a4-5326-4b34-c212-14677128b0c7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample from Training Set:\n",
            "[['mln', 'vs', '.'], ['although', 'these', 'had', 'made', 'minor', 'contribution', 'to', 'profits', 'the', 'real', 'benefits', 'would', 'come', 'in', 'and', 'beyond', '.'], ['we', 'think', 'the', 'stock', 'will', 'do', 'moderately', 'better', 'than', 'the', 'market', 'he', 'said', '.']]\n",
            "\n",
            "Sample from Development Set:\n",
            "[['lt', 'atco', 'ltd', 'sees', 'gain', 'from', 'sale', 'atco', 'ltd', 'said', 'its', 'atco', 'development', 'unit', 'agreed', 'to', 'sell', 'the', 'canadian', 'utilities', 'center', 'in', 'edmonton', 'alberta', 'and', 'the', 'canadian', 'western', 'center', 'in', 'calgary', '.'], ['what', 'i', \"'m\", 'really', 'saying', 'is', 'that', 'they', 'should', 'not', 'expect', 'us', 'to', 'simply', 'sit', 'back', 'here', 'and', 'accept', 'increased', 'tightening', 'on', 'their', 'part', 'on', 'the', 'assumption', 'that', 'somehow', 'we', 'are', 'going', 'to', 'follow', 'them', 'he', 'added', '.'], ['mln', 'marks', 'from', '.']]\n",
            "\n",
            "Sample from Test Set:\n",
            "[['in', 'the', 'fiscal', 'year', 'ended', 'october', 'sulpetro', 'had', 'a', 'net', 'loss', 'of', '.'], ['analysts', 'said', 'air', 'canada', \"'s\", 'immediate', 'concern', 'ahead', 'of', 'a', 'public', 'stock', 'offering', 'will', 'be', 'unloading', 'unprofitable', 'air', 'routes', 'without', 'setting', 'off', 'a', 'political', 'storm', '.'], ['the', 'surplus', 'on', 'the', 'current', 'account', 'narrowed', 'to', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-gram Calculation with Kneser-Ney Smoothing\n",
        "\n",
        "The following functions are used to calculate n-grams (unigrams, bigrams, trigrams, etc.) from a given corpus. Kneser-Ney smoothing is applied to bigrams and higher-order n-grams to improve the model's performance by adjusting for zero-frequency events.\n",
        "\n",
        "### Functions:\n",
        "\n",
        "1. **`calc_ngrams(corpus, n)`**:\n",
        "   - **Purpose:** This function calculates n-grams (where `n` is the number of items in the n-gram) with Kneser-Ney smoothing.\n",
        "   - **How It Works:**\n",
        "     - It creates `n`-grams for each sentence in the corpus and updates a counter to track the frequency of each n-gram.\n",
        "     - It also generates (n-1)-grams for lower-order smoothing.\n",
        "     - If the `n`-grams are bigrams or higher, Kneser-Ney smoothing is applied by adjusting the frequency of n-grams based on the lower-order (n-1)-grams.\n",
        "   - **Return:** A `Counter` object containing the smoothed n-grams.\n",
        "\n",
        "2. **`calc_unigrams(corpus)`**:\n",
        "   - **Purpose:** This function calculates unigrams (single words) from the corpus.\n",
        "   - **How It Works:** It creates unigrams for each sentence in the corpus and counts their occurrences.\n",
        "   - **Return:** A `Counter` object containing the unigrams. No smoothing is applied here because Kneser-Ney smoothing typically applies to bigrams or higher.\n",
        "\n",
        "3. **`calc_bigrams(corpus)`**:\n",
        "   - **Purpose:** This function calculates bigrams (pairs of consecutive words) from the corpus, using the `calc_ngrams` function with `n=2`.\n",
        "   - **Return:** A `Counter` object containing the smoothed bigrams.\n",
        "\n",
        "4. **`calc_trigrams(corpus)`**:\n",
        "   - **Purpose:** This function calculates trigrams (triplets of consecutive words) from the corpus, using the `calc_ngrams` function with `n=3`.\n",
        "   - **Return:** A `Counter` object containing the smoothed trigrams.\n",
        "\n",
        "### Explanation of Kneser-Ney Smoothing:\n",
        "- **Kneser-Ney smoothing** is a technique used to adjust the probabilities of n-grams, especially for those that have never been seen in the training corpus. It reduces the impact of unseen n-grams by distributing probability mass to less frequent n-grams, thus making the model more robust to rare events.\n",
        "- The function first calculates the raw frequency of n-grams, then applies the smoothing technique by modifying the counts of n-grams based on their lower-order counterparts.\n",
        "\n",
        "These functions help in building a language model by counting the frequency of n-grams and applying smoothing to handle unseen combinations of words.\n"
      ],
      "metadata": {
        "id": "4sPftME4GROV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from nltk import ngrams\n",
        "\n",
        "def calc_ngrams(corpus, n):\n",
        "    \"\"\" Returns a Counter for n-grams (unigrams, bigrams, trigrams, etc.) with Kneser-Ney smoothing.\"\"\"\n",
        "    ngram_counter = Counter()\n",
        "    lower_order_counter = Counter()\n",
        "\n",
        "    for sentence in corpus:\n",
        "        # Generate n-grams and update the counter\n",
        "        sentence_ngrams = list(ngrams(sentence, n, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='<e>'))\n",
        "        ngram_counter.update(sentence_ngrams)\n",
        "\n",
        "        # Generate (n-1)-grams for lower-order smoothing\n",
        "        if n > 1:\n",
        "            lower_order_ngrams = list(ngrams(sentence, n-1, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='<e>'))\n",
        "            lower_order_counter.update(lower_order_ngrams)\n",
        "\n",
        "    # Apply Kneser-Ney smoothing for bigrams and higher\n",
        "    if n > 1:\n",
        "        for (gram, count) in ngram_counter.items():\n",
        "            # Bigram smoothing: Subtract 1 from the count, add back the discount (only for seen bigrams)\n",
        "            lower_order_count = lower_order_counter[gram[:-1]]\n",
        "            ngram_counter[gram] = max(count - 1, 0) / lower_order_count\n",
        "\n",
        "    # For unigrams, just return the counts (Kneser-Ney only applies to bigrams or higher)\n",
        "    return ngram_counter\n",
        "\n",
        "def calc_unigrams(corpus):\n",
        "    # Returns a Unigram Counter with Kneser-Ney smoothing.\n",
        "    unigram_counter = Counter()\n",
        "    for sentence in corpus:\n",
        "        unigram_counter.update(ngrams(sentence, 1, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='<e>'))\n",
        "\n",
        "    # For unigrams, no smoothing is applied in Kneser-Ney (this is typically for bigrams or higher)\n",
        "    return unigram_counter\n",
        "\n",
        "def calc_bigrams(corpus):\n",
        "    # Returns a Bigram Counter with Kneser-Ney smoothing.\n",
        "    return calc_ngrams(corpus, 2)\n",
        "\n",
        "def calc_trigrams(corpus):\n",
        "    # Returns a Trigram Counter with Kneser-Ney smoothing.\n",
        "    return calc_ngrams(corpus, 3)\n"
      ],
      "metadata": {
        "id": "pan4O_M3F9FP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replacing Out-of-Vocabulary (OOV) Words in Training Corpus\n",
        "\n",
        "The `replace_oov_words_train` function identifies and replaces out-of-vocabulary (OOV) words in a training corpus. It also creates a vocabulary by excluding the OOV words and returning them separately.\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **Calculate Unigram Counts:**\n",
        "   - The function starts by calculating the unigram (single word) counts for the entire corpus using the `calc_unigrams` function. This gives the frequency of each word in the corpus.\n",
        "\n",
        "2. **Identify OOV Words:**\n",
        "   - OOV words are those whose frequency in the corpus is less than a predefined threshold (in this case, less than 10 occurrences).\n",
        "   - The function creates a dictionary where OOV words are mapped to the placeholder `'UNK'`.\n",
        "\n",
        "3. **Replace OOV Words in the Corpus:**\n",
        "   - The function processes each sentence in the corpus and replaces any OOV word with `'UNK'`. Words that are not OOV are left unchanged.\n",
        "\n",
        "4. **Create Vocabulary:**\n",
        "   - The vocabulary is created by including only those words from the unigram counts that are not considered OOV. The resulting vocabulary excludes the rare words that have been replaced with `'UNK'`.\n",
        "\n",
        "5. **Return Results:**\n",
        "   - The function returns:\n",
        "     - A dictionary of OOV words, where each OOV word is replaced by `'UNK'`.\n",
        "     - The cleaned corpus where OOV words have been replaced with `'UNK'`.\n",
        "     - The vocabulary, which contains all the words that are not considered OOV.\n",
        "\n",
        "### Purpose:\n",
        "The function is useful in preparing a training corpus for language models by replacing rare or unseen words (OOV) with a special token (`'UNK'`). This ensures that the model doesn't encounter issues when faced with words that were not present in the training set. It also creates a vocabulary that helps in efficient processing and model training by excluding rare words.\n"
      ],
      "metadata": {
        "id": "V7oKKCwKGjso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_oov_words_train(corpus):\n",
        "\n",
        "    # Calculate unigram frequencies for the corpus\n",
        "    unigram_counter = calc_unigrams(corpus)\n",
        "\n",
        "    # Create a dictionary for OOV words (those that appear less than 10 times)\n",
        "    OOV_words = {k[0]: \"UNK\" for k, v in unigram_counter.items() if v < 10}\n",
        "\n",
        "    # Replace OOV words in the corpus (using list comprehension for each sentence)\n",
        "    clean_corpus = [\n",
        "        [OOV_words.get(word, word) for word in sentence]\n",
        "        for sentence in corpus\n",
        "    ]\n",
        "\n",
        "    # Create vocabulary (set of unique words not in OOV_words)\n",
        "    vocabulary = [f[0] for f in unigram_counter.keys() if f[0] not in OOV_words]\n",
        "    vocabulary = set(vocabulary)  # Set for unique words\n",
        "\n",
        "    return OOV_words, clean_corpus, vocabulary\n",
        "\n",
        "# Example usage:\n",
        "oov_words, clean_corpus, vocabulary = replace_oov_words_train(train_corpus)\n"
      ],
      "metadata": {
        "id": "N3ZcxbD5ZSWI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_oov_words_dev_test(corpus, vocabulary, oov_words):\n",
        "    clean_corpus = []\n",
        "    for sentence in corpus:\n",
        "        updated_sentence = ['UNK' if ((word not in vocabulary) or (word in oov_words)) else word for word in sentence]\n",
        "        clean_corpus.append(updated_sentence)\n",
        "    return clean_corpus\n",
        "final_dev_corpus = replace_oov_words_dev_test(dev_corpus, vocabulary, oov_words)\n",
        "final_test_corpus = replace_oov_words_dev_test(test_corpus, vocabulary, oov_words)\n"
      ],
      "metadata": {
        "id": "7foqhs-wcxTM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to print samples from the final corpus\n",
        "def print_samples(corpus, num_samples=5):\n",
        "    \"\"\" Print a few sample sentences from the corpus \"\"\"\n",
        "    for i, sentence in enumerate(corpus[:num_samples]):\n",
        "        print(f\"Sample {i+1}: {sentence}\")\n",
        "\n",
        "# Print samples from final_dev_corpus and final_test_corpus\n",
        "print(\"Samples from final_dev_corpus:\")\n",
        "print_samples(final_dev_corpus)\n",
        "\n",
        "print(\"\\nSamples from final_test_corpus:\")\n",
        "print_samples(final_test_corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kN7O0I7PczfI",
        "outputId": "230721b9-4f03-4874-8df0-7bebc12a75d6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples from final_dev_corpus:\n",
            "Sample 1: ['lt', 'UNK', 'ltd', 'sees', 'gain', 'from', 'sale', 'UNK', 'ltd', 'said', 'its', 'UNK', 'development', 'unit', 'agreed', 'to', 'sell', 'the', 'canadian', 'utilities', 'center', 'in', 'edmonton', 'alberta', 'and', 'the', 'canadian', 'western', 'center', 'in', 'UNK', '.']\n",
            "Sample 2: ['what', 'i', \"'m\", 'really', 'saying', 'is', 'that', 'they', 'should', 'not', 'expect', 'us', 'to', 'simply', 'UNK', 'back', 'here', 'and', 'accept', 'increased', 'tightening', 'on', 'their', 'part', 'on', 'the', 'assumption', 'that', 'UNK', 'we', 'are', 'going', 'to', 'follow', 'them', 'he', 'added', '.']\n",
            "Sample 3: ['mln', 'marks', 'from', '.']\n",
            "Sample 4: ['the', 'minister', 'for', 'economic', 'affairs', 'would', 'need', 'to', 'be', 'informed', 'in', 'advance', 'of', 'deals', 'under', 'which', 'foreign', 'interests', 'planned', 'to', 'buy', 'a', 'new', 'stake', 'of', 'more', 'than', 'ten', 'pct', 'of', 'the', 'voting', 'shares', 'in', 'a', 'large', 'belgian', 'company', 'or', 'to', 'increase', 'an', 'existing', 'stake', 'to', 'more', 'than', 'pct', '.']\n",
            "Sample 5: ['the', 'employment', 'data', 'suggests', 'a', 'gnp', 'annual', 'growth', 'rate', 'of', 'about', 'three', 'to', '.']\n",
            "\n",
            "Samples from final_test_corpus:\n",
            "Sample 1: ['in', 'the', 'fiscal', 'year', 'ended', 'october', 'UNK', 'had', 'a', 'net', 'loss', 'of', '.']\n",
            "Sample 2: ['analysts', 'said', 'air', 'canada', \"'s\", 'immediate', 'concern', 'ahead', 'of', 'a', 'public', 'stock', 'offering', 'will', 'be', 'UNK', 'unprofitable', 'air', 'routes', 'without', 'setting', 'off', 'a', 'political', 'UNK', '.']\n",
            "Sample 3: ['the', 'surplus', 'on', 'the', 'current', 'account', 'narrowed', 'to', '.']\n",
            "Sample 4: ['mln', 'vs', '.']\n",
            "Sample 5: ['yen', 'in', 'tokyo', 'brokers', 'dollar', 'trades', 'at', 'post', 'war', 'low', 'of', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Do We Compute the Sum of Logarithms of N-gram Probabilities?\n",
        "\n",
        "In n-gram language models, probabilities are often very small, especially for longer sequences. Instead of directly multiplying these probabilities, we compute the **sum of logarithms** of the probabilities. Here's why this approach is used and why it is important.\n",
        "\n",
        "### 1. **Small Probabilities**\n",
        "\n",
        "In typical n-gram models, the probability of a sequence of words is calculated by multiplying the probabilities of individual n-grams. For example, the probability of a sequence like \"the market is rising\" might be computed as:\n",
        "\n",
        "$$\n",
        "P(\\text{\"the\"}) = 0.1, \\quad P(\\text{\"market\" | the}) = 0.05, \\quad \\dots\n",
        "$$\n",
        "\n",
        "Multiplying these small probabilities together yields an extremely small number, which can cause **numerical underflow** (i.e., the result becomes too small to represent in a computer).\n",
        "\n",
        "### 2. **Logarithms for Numerical Stability**\n",
        "\n",
        "To avoid underflow and to handle very small numbers efficiently, we use **logarithms**. Logarithms have the following useful property:\n",
        "\n",
        "$$\n",
        "\\log(P(w_1, w_2, \\dots, w_n)) = \\log(P(w_1)) + \\log(P(w_2 | w_1)) + \\dots + \\log(P(w_n | w_1, \\dots, w_{n-1}))\n",
        "$$\n",
        "\n",
        "By applying the logarithm, we transform the **product of probabilities** into a **sum of logarithms**. This makes it easier to work with small values and avoids the numerical problems associated with multiplying many small numbers.\n",
        "\n",
        "### 3. **Simplification of Calculations**\n",
        "\n",
        "- **Logarithms** simplify the model's calculations by turning a **multiplicative** process into an **additive** one.\n",
        "- The logarithm of any probability is always **negative** (since probabilities are between 0 and 1), but the sum of these logarithms can be handled much more easily.\n",
        "\n",
        "This transformation allows us to compute the **log-likelihood** of a sequence efficiently. For example, the log-likelihood of a sequence $( w_1, w_2, \\dots, w_n )$ is given by:\n",
        "\n",
        "$$\n",
        "\\text{log-likelihood} = \\sum_{i=1}^{n} \\log P(w_i | w_{i-1}, \\dots, w_1)\n",
        "$$\n",
        "\n",
        "Maximizing the log-likelihood is equivalent to maximizing the likelihood but is much easier to compute.\n",
        "\n",
        "### 4. **Log-Likelihood Maximization**\n",
        "\n",
        "Maximizing the likelihood of a sequence is essential in language modeling, and using logarithms makes this process more computationally feasible. The sum of logarithms allows for easier **optimization**, particularly when using **maximum likelihood estimation (MLE)**."
      ],
      "metadata": {
        "id": "ssFI1Y_DPsVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why We Do the Same\n",
        "\n",
        "- **Numerical Stability**: Multiplying many small probabilities can cause underflow. Taking the log of probabilities ensures that we avoid this issue.\n",
        "- **Simplification**: Adding logarithms is computationally simpler and more stable than multiplying probabilities.\n",
        "- **Optimization**: The log-likelihood function is easier to maximize compared to the product of probabilities.\n",
        "- **Consistency**: Using log-probabilities is the standard approach in machine learning and natural language processing, ensuring consistency with other models and libraries.\n",
        "\n",
        "In summary, computing the sum of the logarithms of the n-gram probabilities instead of directly multiplying the probabilities is a widely adopted practice for maintaining numerical stability and simplifying the optimization process in language models.\n",
        "\n"
      ],
      "metadata": {
        "id": "74g39_YxRfUF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "36acIji-TKnE"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}