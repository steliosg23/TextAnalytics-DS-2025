{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steliosg23/TextAnalytics-DS-2025/blob/main/TA_Assignment_1_N_grams_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3"
      ],
      "metadata": {
        "id": "dxxNYY4nYbGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(i) Implement a bigram and a trigram language model for sentences, using Laplace smoothing or optionally Kneser- Ney smoothing.\n",
        "\n",
        "In practice, n-gram language models\n",
        "compute the sum of the logarithms of the n-gram probabilities of each sequence, instead of\n",
        "their product and you should do the same.\n",
        "\n",
        "Assume that each sentence starts with the\n",
        "pseudo-token *start* (or two pseudo-tokens *start1*, *start2* for the trigram model) and\n",
        "ends with the pseudo-token *end*.\n",
        "\n",
        "Train your models on a training subset of a corpus. Include in the vocabulary\n",
        "only words that occur, e.g., at least 10 times in the training subset. Use the same vocabulary\n",
        "in the bigram and trigram models. Replace all out-of-vocabulary (OOV) words (in the\n",
        "training, development, test subsets) by a special token *UNK*.\n",
        "\n",
        "Alternatively, use BPEs instead of words (obtaining the BPE vocabulary from your training subset) to\n",
        "avoid unknown words."
      ],
      "metadata": {
        "id": "67WIft8qYgIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U nltk"
      ],
      "metadata": {
        "id": "uSB6cg5JDprR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBeJGofPHRew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87134851-0ee2-4c49-c431-cf5c334452d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the Reuters corpus from NLTK library."
      ],
      "metadata": {
        "id": "ynjXqq3PKaNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('reuters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8S-A249MDcl",
        "outputId": "5ed37913-381b-4dff-c167-acb5cc3f216b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the corpus"
      ],
      "metadata": {
        "id": "4JdBbJacKngy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "\n",
        "sentences = []\n",
        "\n",
        "for fileid in reuters.fileids():\n",
        "\n",
        "    doc_sentences = sent_tokenize(reuters.raw(fileid))\n",
        "    sentences.extend(doc_sentences)\n",
        "\n",
        "# Tokenize each sentence\n",
        "# tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "\n",
        "tokenized_sentences = []\n",
        "for sentence in sentences:\n",
        "\n",
        "    tokens = word_tokenize(sentence.lower())\n",
        "\n",
        "    tokens_with_start_end = ['start'] + tokens + ['end']  # For bigrams/unigrams\n",
        "\n",
        "    tokenized_sentences.append(tokens_with_start_end)\n",
        "\n",
        "# Add start1, start2 for trigrams\n",
        "tokenized_sentences_trigrams = []\n",
        "for sentence in tokenized_sentences:\n",
        "\n",
        "    tokens_with_start_end = ['start1', 'start2'] + sentence[1:]  # Add start1, start2\n",
        "\n",
        "    tokenized_sentences_trigrams.append(tokens_with_start_end)\n",
        "\n",
        "\n",
        "print(\"First 20 tokenized sentences:\")\n",
        "for i, sentence in enumerate(tokenized_sentences[:20]):\n",
        "    print(f\"Sentence {i+1}: {sentence}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ2lsuvUKj_n",
        "outputId": "7153651e-ff8f-4652-976a-e5ca93a147f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 tokenized sentences:\n",
            "Sentence 1: ['start', 'asian', 'exporters', 'fear', 'damage', 'from', 'u.s.-japan', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'u.s.', 'and', 'japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'asia', \"'s\", 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far-reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.', 'end']\n",
            "Sentence 2: ['start', 'they', 'told', 'reuter', 'correspondents', 'in', 'asian', 'capitals', 'a', 'u.s.', 'move', 'against', 'japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'u.s.', 'and', 'lead', 'to', 'curbs', 'on', 'american', 'imports', 'of', 'their', 'products', '.', 'end']\n",
            "Sentence 3: ['start', 'but', 'some', 'exporters', 'said', 'that', 'while', 'the', 'conflict', 'would', 'hurt', 'them', 'in', 'the', 'long-run', ',', 'in', 'the', 'short-term', 'tokyo', \"'s\", 'loss', 'might', 'be', 'their', 'gain', '.', 'end']\n",
            "Sentence 4: ['start', 'the', 'u.s.', 'has', 'said', 'it', 'will', 'impose', '300', 'mln', 'dlrs', 'of', 'tariffs', 'on', 'imports', 'of', 'japanese', 'electronics', 'goods', 'on', 'april', '17', ',', 'in', 'retaliation', 'for', 'japan', \"'s\", 'alleged', 'failure', 'to', 'stick', 'to', 'a', 'pact', 'not', 'to', 'sell', 'semiconductors', 'on', 'world', 'markets', 'at', 'below', 'cost', '.', 'end']\n",
            "Sentence 5: ['start', 'unofficial', 'japanese', 'estimates', 'put', 'the', 'impact', 'of', 'the', 'tariffs', 'at', '10', 'billion', 'dlrs', 'and', 'spokesmen', 'for', 'major', 'electronics', 'firms', 'said', 'they', 'would', 'virtually', 'halt', 'exports', 'of', 'products', 'hit', 'by', 'the', 'new', 'taxes', '.', 'end']\n",
            "Sentence 6: ['start', '``', 'we', 'would', \"n't\", 'be', 'able', 'to', 'do', 'business', ',', \"''\", 'said', 'a', 'spokesman', 'for', 'leading', 'japanese', 'electronics', 'firm', 'matsushita', 'electric', 'industrial', 'co', 'ltd', '&', 'lt', ';', 'mc.t', '>', '.', 'end']\n",
            "Sentence 7: ['start', '``', 'if', 'the', 'tariffs', 'remain', 'in', 'place', 'for', 'any', 'length', 'of', 'time', 'beyond', 'a', 'few', 'months', 'it', 'will', 'mean', 'the', 'complete', 'erosion', 'of', 'exports', '(', 'of', 'goods', 'subject', 'to', 'tariffs', ')', 'to', 'the', 'u.s.', ',', \"''\", 'said', 'tom', 'murtha', ',', 'a', 'stock', 'analyst', 'at', 'the', 'tokyo', 'office', 'of', 'broker', '&', 'lt', ';', 'james', 'capel', 'and', 'co', '>', '.', 'end']\n",
            "Sentence 8: ['start', 'in', 'taiwan', ',', 'businessmen', 'and', 'officials', 'are', 'also', 'worried', '.', 'end']\n",
            "Sentence 9: ['start', '``', 'we', 'are', 'aware', 'of', 'the', 'seriousness', 'of', 'the', 'u.s', '.', 'end']\n",
            "Sentence 10: ['start', 'threat', 'against', 'japan', 'because', 'it', 'serves', 'as', 'a', 'warning', 'to', 'us', ',', \"''\", 'said', 'a', 'senior', 'taiwanese', 'trade', 'official', 'who', 'asked', 'not', 'to', 'be', 'named', '.', 'end']\n",
            "Sentence 11: ['start', 'taiwan', 'had', 'a', 'trade', 'trade', 'surplus', 'of', '15.6', 'billion', 'dlrs', 'last', 'year', ',', '95', 'pct', 'of', 'it', 'with', 'the', 'u.s', '.', 'end']\n",
            "Sentence 12: ['start', 'the', 'surplus', 'helped', 'swell', 'taiwan', \"'s\", 'foreign', 'exchange', 'reserves', 'to', '53', 'billion', 'dlrs', ',', 'among', 'the', 'world', \"'s\", 'largest', '.', 'end']\n",
            "Sentence 13: ['start', '``', 'we', 'must', 'quickly', 'open', 'our', 'markets', ',', 'remove', 'trade', 'barriers', 'and', 'cut', 'import', 'tariffs', 'to', 'allow', 'imports', 'of', 'u.s.', 'products', ',', 'if', 'we', 'want', 'to', 'defuse', 'problems', 'from', 'possible', 'u.s', '.', 'end']\n",
            "Sentence 14: ['start', 'retaliation', ',', \"''\", 'said', 'paul', 'sheen', ',', 'chairman', 'of', 'textile', 'exporters', '&', 'lt', ';', 'taiwan', 'safe', 'group', '>', '.', 'end']\n",
            "Sentence 15: ['start', 'a', 'senior', 'official', 'of', 'south', 'korea', \"'s\", 'trade', 'promotion', 'association', 'said', 'the', 'trade', 'dispute', 'between', 'the', 'u.s.', 'and', 'japan', 'might', 'also', 'lead', 'to', 'pressure', 'on', 'south', 'korea', ',', 'whose', 'chief', 'exports', 'are', 'similar', 'to', 'those', 'of', 'japan', '.', 'end']\n",
            "Sentence 16: ['start', 'last', 'year', 'south', 'korea', 'had', 'a', 'trade', 'surplus', 'of', '7.1', 'billion', 'dlrs', 'with', 'the', 'u.s.', ',', 'up', 'from', '4.9', 'billion', 'dlrs', 'in', '1985', '.', 'end']\n",
            "Sentence 17: ['start', 'in', 'malaysia', ',', 'trade', 'officers', 'and', 'businessmen', 'said', 'tough', 'curbs', 'against', 'japan', 'might', 'allow', 'hard-hit', 'producers', 'of', 'semiconductors', 'in', 'third', 'countries', 'to', 'expand', 'their', 'sales', 'to', 'the', 'u.s', '.', 'end']\n",
            "Sentence 18: ['start', 'in', 'hong', 'kong', ',', 'where', 'newspapers', 'have', 'alleged', 'japan', 'has', 'been', 'selling', 'below-cost', 'semiconductors', ',', 'some', 'electronics', 'manufacturers', 'share', 'that', 'view', '.', 'end']\n",
            "Sentence 19: ['start', 'but', 'other', 'businessmen', 'said', 'such', 'a', 'short-term', 'commercial', 'advantage', 'would', 'be', 'outweighed', 'by', 'further', 'u.s.', 'pressure', 'to', 'block', 'imports', '.', 'end']\n",
            "Sentence 20: ['start', '``', 'that', 'is', 'a', 'very', 'short-term', 'view', ',', \"''\", 'said', 'lawrence', 'mills', ',', 'director-general', 'of', 'the', 'federation', 'of', 'hong', 'kong', 'industry', '.', 'end']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Bigrams and Trigrams"
      ],
      "metadata": {
        "id": "1NGLd1b-MN1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_unigrams(tokenized_sentences):\n",
        "    unigrams = []\n",
        "    for sentence in tokenized_sentences:\n",
        "        unigrams.extend(sentence)  # Add each word in the sentence\n",
        "    return unigrams\n",
        "\n",
        "def generate_bigrams(tokenized_sentences):\n",
        "    bigrams = []\n",
        "    for sentence in tokenized_sentences:\n",
        "        bigrams.extend([(sentence[i], sentence[i+1]) for i in range(len(sentence)-1)])  # Pairs of consecutive words\n",
        "    return bigrams\n",
        "\n",
        "def generate_trigrams(tokenized_sentences):\n",
        "    trigrams = []\n",
        "    for sentence in tokenized_sentences:\n",
        "        trigrams.extend([(sentence[i], sentence[i+1], sentence[i+2]) for i in range(len(sentence)-2)])  # Triplets of consecutive words\n",
        "    return trigrams\n",
        "\n",
        "\n",
        "unigrams = generate_unigrams(tokenized_sentences)\n",
        "bigrams = generate_bigrams(tokenized_sentences)\n",
        "# trigrams = generate_trigrams(tokenized_sentences)\n",
        "trigrams = generate_trigrams(tokenized_sentences_trigrams)\n"
      ],
      "metadata": {
        "id": "QP_Gj2czMPmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count the Occurrences of the Unigrams, the Bigrams and Trigrams"
      ],
      "metadata": {
        "id": "pXa5Nng1MhPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "IRFNXK-QXBOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_frequencies(ngrams):\n",
        "    frequency_dict = defaultdict(int)\n",
        "    for ngram in ngrams:\n",
        "        frequency_dict[ngram] += 1\n",
        "    return frequency_dict\n",
        "\n",
        "# Count frequencies of unigrams, bigrams, and trigrams\n",
        "unigram_counts = count_frequencies(unigrams)\n",
        "bigram_counts = count_frequencies(bigrams)\n",
        "trigram_counts = count_frequencies(trigrams)"
      ],
      "metadata": {
        "id": "3rQPzuXTMSQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_most_common(counts, top_n=20):\n",
        "    return sorted(counts.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "# Get the 20 most common unigrams, bigrams, and trigrams\n",
        "common_unigrams = get_most_common(unigram_counts)\n",
        "common_bigrams = get_most_common(bigram_counts)\n",
        "common_trigrams = get_most_common(trigram_counts)\n",
        "\n",
        "\n",
        "print(\"20 Most Common Unigrams:\")\n",
        "for unigram, count in common_unigrams:\n",
        "    print(f\"{unigram}: {count}\")\n",
        "\n",
        "print(\"\\n20 Most Common Bigrams:\")\n",
        "for bigram, count in common_bigrams:\n",
        "    print(f\"{bigram}: {count}\")\n",
        "\n",
        "print(\"\\n20 Most Common Trigrams:\")\n",
        "for trigram, count in common_trigrams:\n",
        "    print(f\"{trigram}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_5lObTgXJcC",
        "outputId": "ba1a330d-a49f-436b-811c-5a0292d458a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20 Most Common Unigrams:\n",
            "the: 69245\n",
            "end: 54755\n",
            "start: 54100\n",
            ",: 53665\n",
            ".: 51066\n",
            "of: 36749\n",
            "to: 36275\n",
            "in: 29217\n",
            "and: 25616\n",
            "said: 25381\n",
            "a: 24724\n",
            "mln: 18598\n",
            "vs: 14332\n",
            "for: 13420\n",
            "dlrs: 12329\n",
            "it: 11100\n",
            "pct: 9771\n",
            "'s: 9635\n",
            "on: 9094\n",
            ";: 8764\n",
            "\n",
            "20 Most Common Bigrams:\n",
            "('.', 'end'): 49389\n",
            "('start', 'the'): 8867\n",
            "('&', 'lt'): 8694\n",
            "('lt', ';'): 8694\n",
            "('said', '.'): 7890\n",
            "('of', 'the'): 6834\n",
            "('in', 'the'): 6756\n",
            "(',', 'the'): 4411\n",
            "('mln', 'dlrs'): 4399\n",
            "('said', 'it'): 4059\n",
            "('mln', 'vs'): 3918\n",
            "('said', 'the'): 3611\n",
            "('start', '``'): 3599\n",
            "(',', \"''\"): 3449\n",
            "('cts', 'vs'): 3311\n",
            "('the', 'company'): 3124\n",
            "('for', 'the'): 2785\n",
            "('he', 'said'): 2503\n",
            "('to', 'the'): 2482\n",
            "('cts', 'net'): 2194\n",
            "\n",
            "20 Most Common Trigrams:\n",
            "('start1', 'start2', 'the'): 8865\n",
            "('&', 'lt', ';'): 8694\n",
            "('said', '.', 'end'): 7889\n",
            "('start1', 'start2', '``'): 3599\n",
            "('start1', 'start2', 'it'): 1770\n",
            "('.', \"''\", 'end'): 1636\n",
            "('start1', 'start2', 'he'): 1592\n",
            "('start1', 'start2', 'in'): 1385\n",
            "('inc', '&', 'lt'): 1352\n",
            "('dlrs', '.', 'end'): 1295\n",
            "('he', 'said', '.'): 1233\n",
            "('the', 'company', 'said'): 1173\n",
            "('start2', 'the', 'company'): 1155\n",
            "('corp', '&', 'lt'): 1074\n",
            "('start1', 'start2', 'but'): 1055\n",
            "('year', '.', 'end'): 1033\n",
            "(',', \"''\", 'he'): 982\n",
            "('start2', 'it', 'said'): 900\n",
            "('start2', 'he', 'said'): 867\n",
            "('pct', '.', 'end'): 796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laplace Smoothing"
      ],
      "metadata": {
        "id": "prR11XBrM1sU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_word_count = 10\n",
        "\n",
        "# Filter the unigram counts to only include words that occur >= 10 times\n",
        "filtered_unigram_counts = {word: count for word, count in unigram_counts.items() if count >= min_word_count}\n",
        "\n",
        "# Recalculate the total words and vocabulary size based on filtered vocabulary\n",
        "filtered_unigrams = list(filtered_unigram_counts.keys())\n",
        "filtered_vocab_size = len(filtered_unigrams)\n",
        "filtered_total_words = sum(filtered_unigram_counts.values())\n",
        "\n",
        "# Filter the n-grams based on the filtered vocabulary\n",
        "filtered_bigrams = [(w1, w2) for (w1, w2) in bigram_counts if w1 in filtered_unigram_counts and w2 in filtered_unigram_counts]\n",
        "filtered_trigrams = [(w1, w2, w3) for (w1, w2, w3) in trigram_counts if w1 in filtered_unigram_counts and w2 in filtered_unigram_counts and w3 in filtered_unigram_counts]"
      ],
      "metadata": {
        "id": "vm-UYXmyMjqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate probabilities with Laplace Smoothing based on filtered vocabulary\n",
        "alpha = 0.01\n",
        "\n",
        "def calculate_unigram_probabilities():\n",
        "    unigram_probabilities = {}\n",
        "    for unigram, count in filtered_unigram_counts.items():\n",
        "        prob = (count + alpha) / (filtered_total_words + alpha * filtered_vocab_size)\n",
        "        unigram_probabilities[unigram] = prob\n",
        "    return unigram_probabilities\n",
        "\n",
        "def calculate_bigram_probabilities():\n",
        "    bigram_probabilities = {}\n",
        "    for bigram, count in bigram_counts.items():\n",
        "        if bigram[0] in filtered_unigram_counts and bigram[1] in filtered_unigram_counts:\n",
        "            # P(w2 | w1) = count(w1, w2) / count(w1)\n",
        "            prob = (count + alpha) / (filtered_unigram_counts[bigram[0]] + alpha * filtered_vocab_size)\n",
        "            bigram_probabilities[bigram] = prob\n",
        "    return bigram_probabilities\n",
        "\n",
        "def calculate_trigram_probabilities():\n",
        "    trigram_probabilities = {}\n",
        "    for trigram, count in trigram_counts.items():\n",
        "        if trigram[0] in filtered_unigram_counts and trigram[1] in filtered_unigram_counts and trigram[2] in filtered_unigram_counts:\n",
        "            # P(w3 | w1, w2) = count(w1, w2, w3) / count(w1, w2)\n",
        "            prob = (count + alpha) / (bigram_counts[trigram[:2]] + alpha * filtered_vocab_size)\n",
        "            trigram_probabilities[trigram] = prob\n",
        "    return trigram_probabilities\n",
        "\n",
        "\n",
        "filtered_unigram_probabilities = calculate_unigram_probabilities()\n",
        "filtered_bigram_probabilities = calculate_bigram_probabilities()\n",
        "filtered_trigram_probabilities = calculate_trigram_probabilities()\n"
      ],
      "metadata": {
        "id": "4B7BPeZ9cq2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_top_probabilities(probabilities, top_n=20):\n",
        "    sorted_probabilities = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
        "    for ngram, prob in sorted_probabilities[:top_n]:\n",
        "        print(f\"{ngram}: {prob:.6f}\")\n",
        "\n",
        "print(\"\\n20 Most Probable Bigrams (with Laplace smoothing):\")\n",
        "print_top_probabilities(filtered_bigram_probabilities)\n",
        "\n",
        "print(\"\\n20 Most Probable Trigrams (with Laplace smoothing):\")\n",
        "print_top_probabilities(filtered_trigram_probabilities)\n"
      ],
      "metadata": {
        "id": "usVAXFsrc5rn",
        "outputId": "521a85ce-ca3c-4cc2-d6e6-8d6ffb610d23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "20 Most Probable Bigrams (with Laplace smoothing):\n",
            "('lt', ';'): 0.990615\n",
            "('&', 'lt'): 0.989712\n",
            "('.', 'end'): 0.965640\n",
            "('4th', 'qtr'): 0.907945\n",
            "('avg', 'shrs'): 0.901002\n",
            "('3rd', 'qtr'): 0.863568\n",
            "('however', ','): 0.847876\n",
            "('u.s', '.'): 0.815933\n",
            "('according', 'to'): 0.812957\n",
            "('note', ':'): 0.805648\n",
            "('1st', 'qtr'): 0.770605\n",
            "('did', 'not'): 0.769877\n",
            "('buffer', 'stock'): 0.769296\n",
            "('subject', 'to'): 0.758767\n",
            "('2nd', 'qtr'): 0.753537\n",
            "('qtly', 'div'): 0.739478\n",
            "('compared', 'with'): 0.710642\n",
            "('due', 'to'): 0.702887\n",
            "('part', 'of'): 0.677700\n",
            "('number', 'of'): 0.668429\n",
            "\n",
            "20 Most Probable Trigrams (with Laplace smoothing):\n",
            "('&', 'lt', ';'): 0.990840\n",
            "('said', '.', 'end'): 0.989791\n",
            "('.', \"''\", 'end'): 0.953175\n",
            "('inc', '&', 'lt'): 0.943891\n",
            "('dlrs', '.', 'end'): 0.938155\n",
            "('corp', '&', 'lt'): 0.930378\n",
            "('year', '.', 'end'): 0.904261\n",
            "('pct', '.', 'end'): 0.904166\n",
            "('1986', '.', 'end'): 0.898805\n",
            "('>', '4th', 'qtr'): 0.883785\n",
            "('added', '.', 'end'): 0.882565\n",
            "('u.s', '.', 'end'): 0.868414\n",
            "('1985', '.', 'end'): 0.861470\n",
            "('mln', 'avg', 'shrs'): 0.860671\n",
            "('share', '.', 'end'): 0.856079\n",
            "('mln', 'note', ':'): 0.855480\n",
            "('>', '3rd', 'qtr'): 0.846552\n",
            "('the', 'end', 'of'): 0.845758\n",
            "('>', '.', 'end'): 0.831290\n",
            "('>', '1st', 'qtr'): 0.828361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Do We Compute the Sum of Logarithms of N-gram Probabilities?\n",
        "\n",
        "In n-gram language models, probabilities are often very small, especially for longer sequences. Instead of directly multiplying these probabilities, we compute the **sum of logarithms** of the probabilities. Here's why this approach is used and why it is important.\n",
        "\n",
        "### 1. **Small Probabilities**\n",
        "\n",
        "In typical n-gram models, the probability of a sequence of words is calculated by multiplying the probabilities of individual n-grams. For example, the probability of a sequence like \"the market is rising\" might be computed as:\n",
        "\n",
        "$$\n",
        "P(\\text{\"the\"}) = 0.1, \\quad P(\\text{\"market\" | the}) = 0.05, \\quad \\dots\n",
        "$$\n",
        "\n",
        "Multiplying these small probabilities together yields an extremely small number, which can cause **numerical underflow** (i.e., the result becomes too small to represent in a computer).\n",
        "\n",
        "### 2. **Logarithms for Numerical Stability**\n",
        "\n",
        "To avoid underflow and to handle very small numbers efficiently, we use **logarithms**. Logarithms have the following useful property:\n",
        "\n",
        "$$\n",
        "\\log(P(w_1, w_2, \\dots, w_n)) = \\log(P(w_1)) + \\log(P(w_2 | w_1)) + \\dots + \\log(P(w_n | w_1, \\dots, w_{n-1}))\n",
        "$$\n",
        "\n",
        "By applying the logarithm, we transform the **product of probabilities** into a **sum of logarithms**. This makes it easier to work with small values and avoids the numerical problems associated with multiplying many small numbers.\n",
        "\n",
        "### 3. **Simplification of Calculations**\n",
        "\n",
        "- **Logarithms** simplify the model's calculations by turning a **multiplicative** process into an **additive** one.\n",
        "- The logarithm of any probability is always **negative** (since probabilities are between 0 and 1), but the sum of these logarithms can be handled much more easily.\n",
        "\n",
        "This transformation allows us to compute the **log-likelihood** of a sequence efficiently. For example, the log-likelihood of a sequence $( w_1, w_2, \\dots, w_n )$ is given by:\n",
        "\n",
        "$$\n",
        "\\text{log-likelihood} = \\sum_{i=1}^{n} \\log P(w_i | w_{i-1}, \\dots, w_1)\n",
        "$$\n",
        "\n",
        "Maximizing the log-likelihood is equivalent to maximizing the likelihood but is much easier to compute.\n",
        "\n",
        "### 4. **Log-Likelihood Maximization**\n",
        "\n",
        "Maximizing the likelihood of a sequence is essential in language modeling, and using logarithms makes this process more computationally feasible. The sum of logarithms allows for easier **optimization**, particularly when using **maximum likelihood estimation (MLE)**."
      ],
      "metadata": {
        "id": "ssFI1Y_DPsVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why We Do the Same\n",
        "\n",
        "- **Numerical Stability**: Multiplying many small probabilities can cause underflow. Taking the log of probabilities ensures that we avoid this issue.\n",
        "- **Simplification**: Adding logarithms is computationally simpler and more stable than multiplying probabilities.\n",
        "- **Optimization**: The log-likelihood function is easier to maximize compared to the product of probabilities.\n",
        "- **Consistency**: Using log-probabilities is the standard approach in machine learning and natural language processing, ensuring consistency with other models and libraries.\n",
        "\n",
        "In summary, computing the sum of the logarithms of the n-gram probabilities instead of directly multiplying the probabilities is a widely adopted practice for maintaining numerical stability and simplifying the optimization process in language models.\n",
        "\n"
      ],
      "metadata": {
        "id": "74g39_YxRfUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding pseudo-tokens"
      ],
      "metadata": {
        "id": "8ENCPDf3SdNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess text for bigram and trigram models\n",
        "def preprocess_for_ngram_models(tokens, ngram_type=[\"bigram\",'trigram']):\n",
        "    processed_sentences = []\n",
        "\n",
        "    # Add start and end tokens\n",
        "    if ngram_type == \"bigram\":\n",
        "        tokens = [\"*start*\"] + tokens + [\"*end*\"]\n",
        "    elif ngram_type == \"trigram\":\n",
        "        tokens = [\"*start1*\", \"*start2*\"] + tokens + [\"*end*\"]\n",
        "\n",
        "    processed_sentences.append(tokens)\n",
        "\n",
        "    return processed_sentences\n",
        "\n",
        "# Process tokens for bigrams\n",
        "bigram_sentences = preprocess_for_ngram_models(tokens, ngram_type=\"bigram\")\n",
        "\n",
        "# Process tokens for trigrams\n",
        "trigram_sentences = preprocess_for_ngram_models(tokens, ngram_type=\"trigram\")\n",
        "\n",
        "print(\" Bigram Sentences:\")\n",
        "for i in range(3):\n",
        "    print(bigram_sentences)\n",
        "\n",
        "print(\"\\n Trigram Sentences:\")\n",
        "for i in range(3):\n",
        "    print(trigram_sentences)\n"
      ],
      "metadata": {
        "id": "3tlMKeD0M4xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "36acIji-TKnE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}