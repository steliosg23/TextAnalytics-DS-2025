{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/steliosg23/TextAnalytics-DS-2025/blob/main/TA_Assignment_1_N_grams_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3"
      ],
      "metadata": {
        "id": "dxxNYY4nYbGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(i) Implement a bigram and a trigram language model for sentences, using Laplace smoothing or optionally Kneser- Ney smoothing.\n",
        "\n",
        "In practice, n-gram language models\n",
        "compute the sum of the logarithms of the n-gram probabilities of each sequence, instead of\n",
        "their product and you should do the same.\n",
        "\n",
        "Assume that each sentence starts with the\n",
        "pseudo-token *start* (or two pseudo-tokens *start1*, *start2* for the trigram model) and\n",
        "ends with the pseudo-token *end*.\n",
        "\n",
        "Train your models on a training subset of a corpus. Include in the vocabulary\n",
        "only words that occur, e.g., at least 10 times in the training subset. Use the same vocabulary\n",
        "in the bigram and trigram models. Replace all out-of-vocabulary (OOV) words (in the\n",
        "training, development, test subsets) by a special token *UNK*.\n",
        "\n",
        "Alternatively, use BPEs instead of words (obtaining the BPE vocabulary from your training subset) to\n",
        "avoid unknown words."
      ],
      "metadata": {
        "id": "67WIft8qYgIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U nltk"
      ],
      "metadata": {
        "id": "uSB6cg5JDprR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b39f733-3b58-472b-feee-b06ee063d818"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cBeJGofPHRew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27748745-d176-4727-d7c0-42e22a4d4aa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the Reuters corpus from NLTK library."
      ],
      "metadata": {
        "id": "ynjXqq3PKaNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('reuters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8S-A249MDcl",
        "outputId": "e6b6fc40-19b4-49e6-9728-a06b0d48474d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the corpus"
      ],
      "metadata": {
        "id": "4JdBbJacKngy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "stelios\n"
      ],
      "metadata": {
        "id": "Roy0Z5fk1-gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import unicodedata\n",
        "import string\n",
        "from collections import Counter\n",
        "# numpy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "DA2r60PL2QLs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and Processing the Reuters Corpus\n",
        "\n",
        "### 1. Importing the Reuters Corpus\n",
        "We begin by importing the Reuters corpus from the `nltk` library:\n",
        "\n"
      ],
      "metadata": {
        "id": "9gNaYPUv_Hzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import reuters\n",
        "\n",
        "# Load the Reuters corpus\n",
        "reuters_corpus = reuters.fileids()\n",
        "\n",
        "# Print the first 10 files\n",
        "print(reuters_corpus[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoTeMTMt5IMD",
        "outputId": "2d26f103-9aac-4cbc-db12-3d15fcf55cfa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['test/14826', 'test/14828', 'test/14829', 'test/14832', 'test/14833', 'test/14839', 'test/14840', 'test/14841', 'test/14842', 'test/14843']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Constructing the Final Corpus\n",
        "We then iterate through each file in the corpus, convert its text to lowercase, and append it to a string that will contain the entire corpus."
      ],
      "metadata": {
        "id": "vjHv4iqF_SpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty string to store the final corpus\n",
        "final_corpus = ''\n",
        "\n",
        "# Loop through each file in the Reuters corpus\n",
        "for corpus in reuters_corpus:\n",
        "    # Load the raw text from the file\n",
        "    text = reuters.raw(fileids=corpus)\n",
        "\n",
        "    # Convert the text to lowercase\n",
        "    lower_text = text.lower()\n",
        "\n",
        "    # Add the lowercase text to the final corpus\n",
        "    final_corpus += lower_text\n",
        "\n",
        "# Print the total number of words in the final corpus\n",
        "print(len(final_corpus.split()))"
      ],
      "metadata": {
        "id": "2CWoIe8K1sO5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b76f5cec-b936-40f1-bc12-152187fe81dd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1378305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Cleaning Function\n",
        "\n",
        "The `text_cleaning` function is designed to clean the input text by removing unwanted characters, keeping only words and sentence-ending punctuation marks (periods, question marks, exclamation points, and apostrophes). It also removes multiple spaces and newline characters."
      ],
      "metadata": {
        "id": "OiPgi8Ni2INy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def text_cleaning(text):\n",
        "\n",
        "    # Remove all characters except letters (both uppercase and lowercase),\n",
        "    # sentence-ending characters (periods, question marks, exclamation marks),\n",
        "    # and apostrophes. Replace unwanted characters with a space.\n",
        "    corpus = re.sub(r'[^a-zA-Z.?!\\']', ' ', text)\n",
        "\n",
        "    # Remove all left square brackets '[' from the text\n",
        "    corpus = corpus.replace('[', '')\n",
        "\n",
        "    # Replace all right square brackets ']' with a period '.'\n",
        "    corpus = corpus.replace(']', '.')\n",
        "\n",
        "    # Remove specific special characters (like $, @, ^, &, *, (, ), €, :, etc.)\n",
        "    # by replacing them with a space.\n",
        "    corpus = re.sub(r'[[]/$@^&*()€:΄,]', ' ', corpus)\n",
        "\n",
        "    # Replace multiple consecutive spaces with a single space\n",
        "    corpus = re.sub(' +', ' ', corpus)\n",
        "\n",
        "    # Replace newline characters '\\n' with a space to maintain continuous text\n",
        "    corpus = corpus.replace('\\n', ' ')\n",
        "\n",
        "    # Return the cleaned text\n",
        "    return corpus\n"
      ],
      "metadata": {
        "id": "Mv1MESLf2Acy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 150 characters of the corpus before cleaning\n",
        "print(f'Before Cleaning: {final_corpus[:150]}...\\n')\n",
        "\n",
        "# Clean the corpus\n",
        "final_corpus = text_cleaning(final_corpus)\n",
        "\n",
        "# Print the first 150 characters of the corpus after cleaning\n",
        "print(f'After Cleaning: {final_corpus[:150]}...')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_k7csJd2Xu4",
        "outputId": "6d86593c-1c6d-4509-ba7e-036eae33a7d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Cleaning: asian exporters fear damage from u.s.-japan rift\n",
            "  mounting trade friction between the\n",
            "  u.s. and japan has raised fears among many of asia's exportin...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-582df6af6b3e>:18: FutureWarning: Possible nested set at position 1\n",
            "  corpus = re.sub(r'[[]/$@^&*()€:΄,]', ' ', corpus)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Cleaning: asian exporters fear damage from u.s. japan rift mounting trade friction between the u.s. and japan has raised fears among many of asia's exporting na...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "def sentence_tokenization(text):\n",
        "    try:\n",
        "        # Tokenizes the input text into a list of sentences using NLTK's sent_tokenize\n",
        "        return nltk.sent_tokenize(text)\n",
        "    except Exception as e:\n",
        "        # Handle any errors that occur during sentence tokenization\n",
        "        print(f\"Error in tokenizing text: {e}\")\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "-Ucm_F0P3Ets"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_list = sentence_tokenization(final_corpus)\n",
        "\n",
        "# Print the first three sentences of the corpus\n",
        "for i in range(10):\n",
        "    print(f\"Sentence {i+1}: {sentence_list[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLE0FJ553H_R",
        "outputId": "24d01df1-7d82-4cd2-da9e-a732f7189f0a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: asian exporters fear damage from u.s. japan rift mounting trade friction between the u.s. and japan has raised fears among many of asia's exporting nations that the row could inflict far reaching economic damage businessmen and officials said.\n",
            "Sentence 2: they told reuter correspondents in asian capitals a u.s. move against japan might boost protectionist sentiment in the u.s. and lead to curbs on american imports of their products.\n",
            "Sentence 3: but some exporters said that while the conflict would hurt them in the long run in the short term tokyo's loss might be their gain.\n",
            "Sentence 4: the u.s. has said it will impose mln dlrs of tariffs on imports of japanese electronics goods on april in retaliation for japan's alleged failure to stick to a pact not to sell semiconductors on world markets at below cost.\n",
            "Sentence 5: unofficial japanese estimates put the impact of the tariffs at billion dlrs and spokesmen for major electronics firms said they would virtually halt exports of products hit by the new taxes.\n",
            "Sentence 6: we wouldn't be able to do business said a spokesman for leading japanese electronics firm matsushita electric industrial co ltd lt mc.t .\n",
            "Sentence 7: if the tariffs remain in place for any length of time beyond a few months it will mean the complete erosion of exports of goods subject to tariffs to the u.s. said tom murtha a stock analyst at the tokyo office of broker lt james capel and co .\n",
            "Sentence 8: in taiwan businessmen and officials are also worried.\n",
            "Sentence 9: we are aware of the seriousness of the u.s. threat against japan because it serves as a warning to us said a senior taiwanese trade official who asked not to be named.\n",
            "Sentence 10: taiwan had a trade trade surplus of .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Tokenization Function\n",
        "\n",
        "The `word_tokenization` function takes an input text and tokenizes it into a list of words using the NLTK library's `word_tokenize` function. It also handles any exceptions that may occur during the tokenization process."
      ],
      "metadata": {
        "id": "uNJLOfcdCo9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "def word_tokenization(text):\n",
        "    try:\n",
        "        # Tokenizes the input text into a list of words using NLTK's word_tokenize\n",
        "        return nltk.word_tokenize(text)\n",
        "    except Exception as e:\n",
        "        # Handle any errors that occur during word tokenization\n",
        "        print(f\"Error in tokenizing text: {e}\")\n",
        "        return []\n"
      ],
      "metadata": {
        "id": "9vpHoyD0CY0M"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply word_tokenization for each sentence in the sentence list\n",
        "words_in_sentences = [word_tokenization(sentence) for sentence in sentence_list]\n",
        "\n",
        "# Print words for the first three sentences in the corpus\n",
        "for i in range(10):\n",
        "    print(f\"Sentence {i+1}: {words_in_sentences[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-0JT0c1CYxo",
        "outputId": "5c72ef73-9355-4722-f309-26a5e2d9c866"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: ['asian', 'exporters', 'fear', 'damage', 'from', 'u.s.', 'japan', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'u.s.', 'and', 'japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'asia', \"'s\", 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', 'reaching', 'economic', 'damage', 'businessmen', 'and', 'officials', 'said', '.']\n",
            "Sentence 2: ['they', 'told', 'reuter', 'correspondents', 'in', 'asian', 'capitals', 'a', 'u.s.', 'move', 'against', 'japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'u.s.', 'and', 'lead', 'to', 'curbs', 'on', 'american', 'imports', 'of', 'their', 'products', '.']\n",
            "Sentence 3: ['but', 'some', 'exporters', 'said', 'that', 'while', 'the', 'conflict', 'would', 'hurt', 'them', 'in', 'the', 'long', 'run', 'in', 'the', 'short', 'term', 'tokyo', \"'s\", 'loss', 'might', 'be', 'their', 'gain', '.']\n",
            "Sentence 4: ['the', 'u.s.', 'has', 'said', 'it', 'will', 'impose', 'mln', 'dlrs', 'of', 'tariffs', 'on', 'imports', 'of', 'japanese', 'electronics', 'goods', 'on', 'april', 'in', 'retaliation', 'for', 'japan', \"'s\", 'alleged', 'failure', 'to', 'stick', 'to', 'a', 'pact', 'not', 'to', 'sell', 'semiconductors', 'on', 'world', 'markets', 'at', 'below', 'cost', '.']\n",
            "Sentence 5: ['unofficial', 'japanese', 'estimates', 'put', 'the', 'impact', 'of', 'the', 'tariffs', 'at', 'billion', 'dlrs', 'and', 'spokesmen', 'for', 'major', 'electronics', 'firms', 'said', 'they', 'would', 'virtually', 'halt', 'exports', 'of', 'products', 'hit', 'by', 'the', 'new', 'taxes', '.']\n",
            "Sentence 6: ['we', 'would', \"n't\", 'be', 'able', 'to', 'do', 'business', 'said', 'a', 'spokesman', 'for', 'leading', 'japanese', 'electronics', 'firm', 'matsushita', 'electric', 'industrial', 'co', 'ltd', 'lt', 'mc.t', '.']\n",
            "Sentence 7: ['if', 'the', 'tariffs', 'remain', 'in', 'place', 'for', 'any', 'length', 'of', 'time', 'beyond', 'a', 'few', 'months', 'it', 'will', 'mean', 'the', 'complete', 'erosion', 'of', 'exports', 'of', 'goods', 'subject', 'to', 'tariffs', 'to', 'the', 'u.s.', 'said', 'tom', 'murtha', 'a', 'stock', 'analyst', 'at', 'the', 'tokyo', 'office', 'of', 'broker', 'lt', 'james', 'capel', 'and', 'co', '.']\n",
            "Sentence 8: ['in', 'taiwan', 'businessmen', 'and', 'officials', 'are', 'also', 'worried', '.']\n",
            "Sentence 9: ['we', 'are', 'aware', 'of', 'the', 'seriousness', 'of', 'the', 'u.s.', 'threat', 'against', 'japan', 'because', 'it', 'serves', 'as', 'a', 'warning', 'to', 'us', 'said', 'a', 'senior', 'taiwanese', 'trade', 'official', 'who', 'asked', 'not', 'to', 'be', 'named', '.']\n",
            "Sentence 10: ['taiwan', 'had', 'a', 'trade', 'trade', 'surplus', 'of', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Splitting Process\n",
        "\n",
        "In this process, the corpus is split into three subsets: `train_corpus`, `dev_corpus`, and `test_corpus`. This is a common practice in machine learning for training, development, and testing models.\n",
        "\n",
        "### Steps:\n",
        "1. **Set Random Seed for Reproducibility:**\n",
        "   The random seed is set to ensure that the splitting process is reproducible. This means that every time the code runs, the same splits will occur, providing consistency in results.\n",
        "\n",
        "2. **Initial Split into Train and Temporary Corpus:**\n",
        "   The corpus is first split into two sets: the `train_corpus` and a `temp_corpus` which contains 40% of the data. The remaining 60% is used for training. This ensures that a portion of the data is available for training the model.\n",
        "\n",
        "3. **Further Split the Temporary Corpus into Dev and Test Sets:**\n",
        "   The `temp_corpus` (which is 40% of the original corpus) is further split evenly into two parts: the `dev_corpus` (development set) and the `test_corpus` (test set). Each of these subsets will contain 20% of the original corpus. The dev set is used for model tuning, and the test set is kept for evaluating the model's final performance.\n",
        "\n",
        "4. **Resulting Subsets:**\n",
        "   After the splits, you have:\n",
        "   - `train_corpus` containing 60% of the data used for training.\n",
        "   - `dev_corpus` containing 20% of the data used for validation and hyperparameter tuning.\n",
        "   - `test_corpus` containing 20% of the data used for final testing.\n",
        "\n",
        "This splitting process helps in ensuring that the model is trained, validated, and tested on separate data sets to avoid overfitting and ensure robust evaluation.\n"
      ],
      "metadata": {
        "id": "US0LBv6VETXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Shuffle and split the corpus into train, dev, and test sets\n",
        "train_corpus, temp_corpus = train_test_split(words_in_sentences, test_size=0.4, random_state=2025)\n",
        "\n",
        "# Further split temp_corpus into dev and test sets\n",
        "dev_corpus, test_corpus = train_test_split(temp_corpus, test_size=0.5, random_state=2025)\n",
        "\n"
      ],
      "metadata": {
        "id": "mf9M8a3xEqp4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a sample of each set\n",
        "print(\"Sample from Training Set:\")\n",
        "print(train_corpus[:3])  # Printing the first 3 samples from the training set\n",
        "\n",
        "print(\"\\nSample from Development Set:\")\n",
        "print(dev_corpus[:3])  # Printing the first 3 samples from the development set\n",
        "\n",
        "print(\"\\nSample from Test Set:\")\n",
        "print(test_corpus[:3])  # Printing the first 3 samples from the test set\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1k3j5XRFAzf",
        "outputId": "9eefe738-9bc4-4a62-8c1b-3574ece1c1e1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample from Training Set:\n",
            "[['mln', 'vs', '.'], ['although', 'these', 'had', 'made', 'minor', 'contribution', 'to', 'profits', 'the', 'real', 'benefits', 'would', 'come', 'in', 'and', 'beyond', '.'], ['we', 'think', 'the', 'stock', 'will', 'do', 'moderately', 'better', 'than', 'the', 'market', 'he', 'said', '.']]\n",
            "\n",
            "Sample from Development Set:\n",
            "[['lt', 'atco', 'ltd', 'sees', 'gain', 'from', 'sale', 'atco', 'ltd', 'said', 'its', 'atco', 'development', 'unit', 'agreed', 'to', 'sell', 'the', 'canadian', 'utilities', 'center', 'in', 'edmonton', 'alberta', 'and', 'the', 'canadian', 'western', 'center', 'in', 'calgary', '.'], ['what', 'i', \"'m\", 'really', 'saying', 'is', 'that', 'they', 'should', 'not', 'expect', 'us', 'to', 'simply', 'sit', 'back', 'here', 'and', 'accept', 'increased', 'tightening', 'on', 'their', 'part', 'on', 'the', 'assumption', 'that', 'somehow', 'we', 'are', 'going', 'to', 'follow', 'them', 'he', 'added', '.'], ['mln', 'marks', 'from', '.']]\n",
            "\n",
            "Sample from Test Set:\n",
            "[['in', 'the', 'fiscal', 'year', 'ended', 'october', 'sulpetro', 'had', 'a', 'net', 'loss', 'of', '.'], ['analysts', 'said', 'air', 'canada', \"'s\", 'immediate', 'concern', 'ahead', 'of', 'a', 'public', 'stock', 'offering', 'will', 'be', 'unloading', 'unprofitable', 'air', 'routes', 'without', 'setting', 'off', 'a', 'political', 'storm', '.'], ['the', 'surplus', 'on', 'the', 'current', 'account', 'narrowed', 'to', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-gram Calculation with Kneser-Ney Smoothing\n",
        "\n",
        "The following functions are used to calculate n-grams (unigrams, bigrams, trigrams, etc.) from a given corpus. Kneser-Ney smoothing is applied to bigrams and higher-order n-grams to improve the model's performance by adjusting for zero-frequency events.\n",
        "\n",
        "### Functions:\n",
        "\n",
        "1. **`calc_ngrams(corpus, n)`**:\n",
        "   - **Purpose:** This function calculates n-grams (where `n` is the number of items in the n-gram) with Kneser-Ney smoothing.\n",
        "   - **How It Works:**\n",
        "     - It creates `n`-grams for each sentence in the corpus and updates a counter to track the frequency of each n-gram.\n",
        "     - It also generates (n-1)-grams for lower-order smoothing.\n",
        "     - If the `n`-grams are bigrams or higher, Kneser-Ney smoothing is applied by adjusting the frequency of n-grams based on the lower-order (n-1)-grams.\n",
        "   - **Return:** A `Counter` object containing the smoothed n-grams.\n",
        "\n",
        "2. **`calc_unigrams(corpus)`**:\n",
        "   - **Purpose:** This function calculates unigrams (single words) from the corpus.\n",
        "   - **How It Works:** It creates unigrams for each sentence in the corpus and counts their occurrences.\n",
        "   - **Return:** A `Counter` object containing the unigrams. No smoothing is applied here because Kneser-Ney smoothing typically applies to bigrams or higher.\n",
        "\n",
        "3. **`calc_bigrams(corpus)`**:\n",
        "   - **Purpose:** This function calculates bigrams (pairs of consecutive words) from the corpus, using the `calc_ngrams` function with `n=2`.\n",
        "   - **Return:** A `Counter` object containing the smoothed bigrams.\n",
        "\n",
        "4. **`calc_trigrams(corpus)`**:\n",
        "   - **Purpose:** This function calculates trigrams (triplets of consecutive words) from the corpus, using the `calc_ngrams` function with `n=3`.\n",
        "   - **Return:** A `Counter` object containing the smoothed trigrams.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4sPftME4GROV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from tqdm import tqdm\n",
        "\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from tqdm import tqdm\n",
        "\n",
        "def calc_ngrams(corpus, n):\n",
        "    \"\"\" Returns a Counter for n-grams (unigrams, bigrams, trigrams, etc.) without smoothing and log-probabilities. \"\"\"\n",
        "    ngram_counter = Counter()\n",
        "\n",
        "    # Generate n-grams for each sentence with a progress bar\n",
        "    for sentence in tqdm(corpus, desc=f\"Processing {n}-grams\", unit=\"sentence\"):\n",
        "        if n == 3:\n",
        "            # For trigrams, use two start tokens (\"<s1>\", \"<s2>\")\n",
        "            sentence_ngrams = list(ngrams(sentence, n, pad_left=True, pad_right=True, left_pad_symbol='<s1>', right_pad_symbol='<e>'))\n",
        "            # Replace the first two padding symbols for trigrams (<s1>, <s1>) with (<s1>, <s2>)\n",
        "            sentence_ngrams = [(('<s1>', '<s2>') if gram[:2] == ('<s1>', '<s1>') else gram) for gram in sentence_ngrams]\n",
        "        else:\n",
        "            # For other n-grams, use a single start token (\"<s>\")\n",
        "            sentence_ngrams = list(ngrams(sentence, n, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='<e>'))\n",
        "\n",
        "        ngram_counter.update(sentence_ngrams)\n",
        "\n",
        "    # Return the n-gram counts without smoothing or log-probabilities\n",
        "    return ngram_counter\n",
        "\n",
        "def calc_unigrams(corpus):\n",
        "    # Returns a Unigram Counter with Kneser-Ney smoothing.\n",
        "    unigram_counter = Counter()\n",
        "    for sentence in corpus:\n",
        "        unigram_counter.update(ngrams(sentence, 1, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='<e>'))\n",
        "\n",
        "    # For unigrams, no smoothing is applied in Kneser-Ney (this is typically for bigrams or higher)\n",
        "    return unigram_counter\n",
        "\n",
        "def calc_bigrams(corpus):\n",
        "    # Returns a Bigram Counter with Kneser-Ney smoothing.\n",
        "    return calc_ngrams(corpus, 2)\n",
        "\n",
        "def calc_trigrams(corpus):\n",
        "    # Returns a Trigram Counter with Kneser-Ney smoothing.\n",
        "    return calc_ngrams(corpus, 3)\n"
      ],
      "metadata": {
        "id": "pan4O_M3F9FP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replacing Out-of-Vocabulary (OOV) Words in Training Corpus\n",
        "\n",
        "The `replace_oov_words_train` function identifies and replaces out-of-vocabulary (OOV) words in a training corpus. It also creates a vocabulary by excluding the OOV words and returning them separately.\n",
        "\n",
        "### Steps:\n",
        "\n",
        "1. **Calculate Unigram Counts:**\n",
        "   - The function starts by calculating the unigram (single word) counts for the entire corpus using the `calc_unigrams` function. This gives the frequency of each word in the corpus.\n",
        "\n",
        "2. **Identify OOV Words:**\n",
        "   - OOV words are those whose frequency in the corpus is less than a predefined threshold (in this case, less than 10 occurrences).\n",
        "   - The function creates a dictionary where OOV words are mapped to the placeholder `'UNK'`.\n",
        "\n",
        "3. **Replace OOV Words in the Corpus:**\n",
        "   - The function processes each sentence in the corpus and replaces any OOV word with `'UNK'`. Words that are not OOV are left unchanged.\n",
        "\n",
        "4. **Create Vocabulary:**\n",
        "   - The vocabulary is created by including only those words from the unigram counts that are not considered OOV. The resulting vocabulary excludes the rare words that have been replaced with `'UNK'`.\n",
        "\n",
        "5. **Return Results:**\n",
        "   - The function returns:\n",
        "     - A dictionary of OOV words, where each OOV word is replaced by `'UNK'`.\n",
        "     - The cleaned corpus where OOV words have been replaced with `'UNK'`.\n",
        "     - The vocabulary, which contains all the words that are not considered OOV.\n",
        "\n",
        "### Purpose:\n",
        "The function is useful in preparing a training corpus for language models by replacing rare or unseen words (OOV) with a special token (`'UNK'`). This ensures that the model doesn't encounter issues when faced with words that were not present in the training set. It also creates a vocabulary that helps in efficient processing and model training by excluding rare words.\n"
      ],
      "metadata": {
        "id": "V7oKKCwKGjso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why We Do the Same\n",
        "\n",
        "- **Numerical Stability**: Multiplying many small probabilities can cause underflow. Taking the log of probabilities ensures that we avoid this issue.\n",
        "- **Simplification**: Adding logarithms is computationally simpler and more stable than multiplying probabilities.\n",
        "- **Optimization**: The log-likelihood function is easier to maximize compared to the product of probabilities.\n",
        "- **Consistency**: Using log-probabilities is the standard approach in machine learning and natural language processing, ensuring consistency with other models and libraries.\n",
        "\n",
        "In summary, computing the sum of the logarithms of the n-gram probabilities instead of directly multiplying the probabilities is a widely adopted practice for maintaining numerical stability and simplifying the optimization process in language models.\n",
        "\n"
      ],
      "metadata": {
        "id": "74g39_YxRfUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_oov_words_train(corpus):\n",
        "\n",
        "    # Calculate unigram frequencies for the corpus\n",
        "    unigram_counter = calc_unigrams(corpus)\n",
        "\n",
        "    # Create a dictionary for OOV words (those that appear less than 10 times)\n",
        "    OOV_words = {k[0]: \"UNK\" for k, v in unigram_counter.items() if v < 10}\n",
        "\n",
        "    # Replace OOV words in the corpus (using list comprehension for each sentence)\n",
        "    clean_corpus = [\n",
        "        [OOV_words.get(word, word) for word in sentence]\n",
        "        for sentence in corpus\n",
        "    ]\n",
        "\n",
        "    # Create vocabulary (set of unique words not in OOV_words)\n",
        "    vocabulary = [f[0] for f in unigram_counter.keys() if f[0] not in OOV_words]\n",
        "    vocabulary = set(vocabulary)  # Set for unique words\n",
        "\n",
        "    return OOV_words, clean_corpus, vocabulary\n",
        "\n",
        "# Example usage:\n",
        "oov_words, final_train_corpus, vocabulary = replace_oov_words_train(train_corpus)\n"
      ],
      "metadata": {
        "id": "N3ZcxbD5ZSWI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_oov_words_dev_test(corpus, vocabulary, oov_words):\n",
        "    # Define a helper function to replace OOV words in a sentence\n",
        "    def replace_in_sentence(sentence):\n",
        "        return [\n",
        "            'UNK' if word not in vocabulary or word in oov_words else word\n",
        "            for word in sentence\n",
        "        ]\n",
        "\n",
        "    # Apply the replacement to the entire corpus using list comprehension\n",
        "    clean_corpus = list(map(replace_in_sentence, corpus))\n",
        "\n",
        "    return clean_corpus\n",
        "\n",
        "# Example usage:\n",
        "final_dev_corpus = replace_oov_words_dev_test(dev_corpus, vocabulary, oov_words)\n",
        "final_test_corpus = replace_oov_words_dev_test(test_corpus, vocabulary, oov_words)\n"
      ],
      "metadata": {
        "id": "7foqhs-wcxTM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to print samples from the final corpus\n",
        "def print_samples(corpus, num_samples=5):\n",
        "    \"\"\" Print a few sample sentences from the corpus \"\"\"\n",
        "    for i, sentence in enumerate(corpus[:num_samples]):\n",
        "        print(f\"Sample {i+1}: {sentence}\")\n",
        "\n",
        "# Print samples from final_dev_corpus and final_test_corpus\n",
        "print(\"Samples from final_dev_corpus:\")\n",
        "print_samples(final_dev_corpus)\n",
        "\n",
        "print(\"\\nSamples from final_test_corpus:\")\n",
        "print_samples(final_test_corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kN7O0I7PczfI",
        "outputId": "6464e8a0-7360-40d6-bcbd-5a1a3ffcb268"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples from final_dev_corpus:\n",
            "Sample 1: ['lt', 'UNK', 'ltd', 'sees', 'gain', 'from', 'sale', 'UNK', 'ltd', 'said', 'its', 'UNK', 'development', 'unit', 'agreed', 'to', 'sell', 'the', 'canadian', 'utilities', 'center', 'in', 'edmonton', 'alberta', 'and', 'the', 'canadian', 'western', 'center', 'in', 'UNK', '.']\n",
            "Sample 2: ['what', 'i', \"'m\", 'really', 'saying', 'is', 'that', 'they', 'should', 'not', 'expect', 'us', 'to', 'simply', 'UNK', 'back', 'here', 'and', 'accept', 'increased', 'tightening', 'on', 'their', 'part', 'on', 'the', 'assumption', 'that', 'UNK', 'we', 'are', 'going', 'to', 'follow', 'them', 'he', 'added', '.']\n",
            "Sample 3: ['mln', 'marks', 'from', '.']\n",
            "Sample 4: ['the', 'minister', 'for', 'economic', 'affairs', 'would', 'need', 'to', 'be', 'informed', 'in', 'advance', 'of', 'deals', 'under', 'which', 'foreign', 'interests', 'planned', 'to', 'buy', 'a', 'new', 'stake', 'of', 'more', 'than', 'ten', 'pct', 'of', 'the', 'voting', 'shares', 'in', 'a', 'large', 'belgian', 'company', 'or', 'to', 'increase', 'an', 'existing', 'stake', 'to', 'more', 'than', 'pct', '.']\n",
            "Sample 5: ['the', 'employment', 'data', 'suggests', 'a', 'gnp', 'annual', 'growth', 'rate', 'of', 'about', 'three', 'to', '.']\n",
            "\n",
            "Samples from final_test_corpus:\n",
            "Sample 1: ['in', 'the', 'fiscal', 'year', 'ended', 'october', 'UNK', 'had', 'a', 'net', 'loss', 'of', '.']\n",
            "Sample 2: ['analysts', 'said', 'air', 'canada', \"'s\", 'immediate', 'concern', 'ahead', 'of', 'a', 'public', 'stock', 'offering', 'will', 'be', 'UNK', 'unprofitable', 'air', 'routes', 'without', 'setting', 'off', 'a', 'political', 'UNK', '.']\n",
            "Sample 3: ['the', 'surplus', 'on', 'the', 'current', 'account', 'narrowed', 'to', '.']\n",
            "Sample 4: ['mln', 'vs', '.']\n",
            "Sample 5: ['yen', 'in', 'tokyo', 'brokers', 'dollar', 'trades', 'at', 'post', 'war', 'low', 'of', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_ngrams(corpus):\n",
        "    return {\n",
        "        \"unigrams\": calc_unigrams(corpus),\n",
        "        \"bigrams\": calc_bigrams(corpus),\n",
        "        \"trigrams\": calc_trigrams(corpus),\n",
        "    }\n",
        "\n",
        "# Calculate vocabulary length\n",
        "vocabulary_length = len(vocabulary)\n",
        "\n",
        "# Calculate n-gram counters\n",
        "ngram_counters = calculate_ngrams(final_train_corpus)\n",
        "\n",
        "# Access specific n-gram counts as needed\n",
        "unigram_counter = ngram_counters[\"unigrams\"]\n",
        "bigram_counter = ngram_counters[\"bigrams\"]\n",
        "trigram_counter = ngram_counters[\"trigrams\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LZng-JPv0Lr",
        "outputId": "747b40dd-a58d-4767-cb76-abc6d09c9bbd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing 2-grams: 100%|██████████| 48409/48409 [00:00<00:00, 62302.12sentence/s]\n",
            "Processing 3-grams: 100%|██████████| 48409/48409 [00:01<00:00, 39445.70sentence/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def display_ngram_statistics(unigram_counter, bigram_counter, trigram_counter):\n",
        "    # Calculate the vocabulary size from the unigram counter\n",
        "    vocabulary_size = len(set(unigram_counter.elements()))  # Using unique elements in unigram counter\n",
        "    print(f'Vocabulary Size: {vocabulary_size}')\n",
        "    print('-----------------')\n",
        "\n",
        "    # Display the top 10 unigrams and their counts\n",
        "    print('Top 10 Most Frequent Unigrams:')\n",
        "    for unigram, count in unigram_counter.most_common(10):\n",
        "        print(f'{unigram}: {count}')\n",
        "    print('-----------------')\n",
        "\n",
        "    # Display the top 10 bigrams and their counts\n",
        "    print('Top 10 Most Frequent Bigrams:')\n",
        "    for bigram, count in bigram_counter.most_common(10):\n",
        "        print(f'{bigram}: {count}')\n",
        "    print('-----------------')\n",
        "\n",
        "    # Display the top 10 trigrams and their counts\n",
        "    print('Top 10 Most Frequent Trigrams:')\n",
        "    for trigram, count in trigram_counter.most_common(10):\n",
        "        print(f'{trigram}: {count}')\n",
        "    print('-----------------')\n",
        "\n",
        "\n",
        "\n",
        "display_ngram_statistics(unigram_counter, bigram_counter, trigram_counter)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vSswMZZxgvw",
        "outputId": "9e9efb8a-3445-4011-81bc-7ae787346717"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 5148\n",
            "-----------------\n",
            "Top 10 Most Frequent Unigrams:\n",
            "('.',): 48392\n",
            "('UNK',): 46029\n",
            "('the',): 41709\n",
            "('of',): 22093\n",
            "('to',): 21837\n",
            "('in',): 17425\n",
            "('and',): 15444\n",
            "('said',): 15196\n",
            "('a',): 14665\n",
            "('mln',): 11162\n",
            "-----------------\n",
            "Top 10 Most Frequent Bigrams:\n",
            "('.', '<e>'): 48387\n",
            "('<s>', 'mln'): 7566\n",
            "('<s>', 'the'): 5619\n",
            "('said', '.'): 4759\n",
            "('UNK', 'UNK'): 4489\n",
            "('of', 'the'): 4178\n",
            "('in', 'the'): 4054\n",
            "('lt', 'UNK'): 4054\n",
            "('<s>', 'UNK'): 3658\n",
            "('vs', '.'): 3133\n",
            "-----------------\n",
            "Top 10 Most Frequent Trigrams:\n",
            "('<s1>', '<s2>'): 48409\n",
            "('.', '<e>', '<e>'): 48387\n",
            "('said', '.', '<e>'): 4759\n",
            "('vs', '.', '<e>'): 3133\n",
            "('UNK', '.', '<e>'): 2251\n",
            "('<s1>', 'mln', 'vs'): 2250\n",
            "('mln', 'vs', '.'): 1982\n",
            "('to', '.', '<e>'): 1615\n",
            "('of', '.', '<e>'): 1355\n",
            "('<s1>', 'mln', 'dlrs'): 1122\n",
            "-----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def calc_prob(counter, context, word, alpha, vocab_len):\n",
        "    \"\"\"\n",
        "    Generalized function to calculate probabilities with Laplace smoothing for n-grams.\n",
        "\n",
        "    Args:\n",
        "        counter: The n-gram counter (bigram or trigram).\n",
        "        context: A tuple representing the context (word1, word2, ...) of the n-gram.\n",
        "        word: The word whose probability we're calculating.\n",
        "        alpha: The smoothing parameter.\n",
        "        vocab_len: The length of the vocabulary.\n",
        "\n",
        "    Returns:\n",
        "        A float representing the probability of the n-gram.\n",
        "    \"\"\"\n",
        "    # Default count of 0 for missing n-grams\n",
        "    ngram_count = counter.get(context + (word,), 0)\n",
        "    context_count = counter.get(context, 0)\n",
        "\n",
        "    return (ngram_count + alpha) / (context_count + alpha * vocab_len)\n",
        "\n",
        "def calc_bi_prob(word1, word2, alpha, bigram_counter, unigram_counter, vocabulary_length):\n",
        "    \"\"\"\n",
        "    Function that calculates the Bigram model's probability using Laplace smoothing.\n",
        "\n",
        "    Args:\n",
        "        word1: The first word in the bigram.\n",
        "        word2: The second word in the bigram.\n",
        "        alpha: The smoothing parameter.\n",
        "        bigram_counter: Counter for bigrams.\n",
        "        unigram_counter: Counter for unigrams.\n",
        "        vocabulary_length: The length of the vocabulary.\n",
        "\n",
        "    Returns:\n",
        "        A float representing the bigram probability.\n",
        "    \"\"\"\n",
        "    return calc_prob(bigram_counter, (word1,), word2, alpha, vocabulary_length)\n",
        "\n",
        "def calc_tri_prob(word1, word2, word3, alpha, trigram_counter, bigram_counter, vocabulary_length):\n",
        "    \"\"\"\n",
        "    Function that calculates the Trigram model's probability using Laplace smoothing.\n",
        "\n",
        "    Args:\n",
        "        word1: The first word in the trigram.\n",
        "        word2: The second word in the trigram.\n",
        "        word3: The third word in the trigram.\n",
        "        alpha: The smoothing parameter.\n",
        "        trigram_counter: Counter for trigrams.\n",
        "        bigram_counter: Counter for bigrams.\n",
        "        vocabulary_length: The length of the vocabulary.\n",
        "\n",
        "    Returns:\n",
        "        A float representing the trigram probability.\n",
        "    \"\"\"\n",
        "    return calc_prob(trigram_counter, (word1, word2), word3, alpha, vocabulary_length)\n"
      ],
      "metadata": {
        "id": "36acIji-TKnE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Initialize variables\n",
        "alpha_values = np.linspace(0.001, 0.1, 100)\n",
        "\n",
        "# Define function to calculate probabilities for bigrams and trigrams\n",
        "def calc_ngram_prob(model_type, *words, alpha, ngram_counter, context_counter, vocab_len):\n",
        "    if model_type == 'bigram':\n",
        "        return calc_bi_prob(words[0], words[1], alpha, ngram_counter, context_counter, vocab_len)\n",
        "    elif model_type == 'trigram':\n",
        "        return calc_tri_prob(words[0], words[1], words[2], alpha, ngram_counter, context_counter, vocab_len)\n",
        "\n",
        "# Function to calculate perplexity and entropy for a given model type\n",
        "def calculate_perplexity_and_entropy(model_type):\n",
        "    perplexities = []\n",
        "    entropies = []\n",
        "    best_alpha = None\n",
        "    min_entropy = float('inf')\n",
        "\n",
        "    # Iterate over alpha values for smoothing\n",
        "    for alpha in alpha_values:\n",
        "        log_prob_sum = 0\n",
        "        ngram_count = 0\n",
        "\n",
        "        # Iterate through the development corpus\n",
        "        for sentence in dev_corpus:\n",
        "            sentence = ['<s>'] + sentence + ['<e>']  # Add start and end tokens\n",
        "            for idx in range(1, len(sentence)):\n",
        "                if model_type == 'bigram':\n",
        "                    ngram_prob = calc_ngram_prob(model_type, sentence[idx-1], sentence[idx], alpha=alpha,\n",
        "                                                  ngram_counter=bigram_counter, context_counter=unigram_counter,\n",
        "                                                  vocab_len=vocabulary_length)\n",
        "                elif model_type == 'trigram' and idx < len(sentence) - 1:\n",
        "                    ngram_prob = calc_ngram_prob(model_type, sentence[idx-1], sentence[idx], sentence[idx+1], alpha=alpha,\n",
        "                                                  ngram_counter=trigram_counter, context_counter=bigram_counter,\n",
        "                                                  vocab_len=vocabulary_length)\n",
        "                log_prob_sum += math.log2(ngram_prob)\n",
        "                ngram_count += 1\n",
        "\n",
        "        # Calculate entropy and perplexity for the current alpha\n",
        "        avg_entropy = -log_prob_sum / ngram_count\n",
        "        entropies.append(avg_entropy)\n",
        "        perplexities.append(2 ** avg_entropy)\n",
        "\n",
        "        # Track the minimum entropy and corresponding alpha\n",
        "        if avg_entropy < min_entropy:\n",
        "            min_entropy = avg_entropy\n",
        "            best_alpha = alpha\n",
        "\n",
        "    return min(perplexities), min(entropies), best_alpha\n",
        "\n",
        "# Calculate and print results for both bigram and trigram models\n",
        "for model_type in ['bigram', 'trigram']:\n",
        "    min_perplexity, min_entropy, best_alpha = calculate_perplexity_and_entropy(model_type)\n",
        "\n",
        "    print(f\"The lowest perplexity/entropy regarding the {model_type} model is: \"\n",
        "          f\"{round(min_perplexity, 3)} / {round(min_entropy, 3)} at alpha = {np.round(best_alpha, 3)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf7AH_XOgyoz",
        "outputId": "6f353c27-16ae-4cf0-b99b-18483a92845d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lowest perplexity/entropy regarding the bigram model is: 0.941 / -0.088 at alpha = 0.001\n",
            "The lowest perplexity/entropy regarding the trigram model is: 56.214 / 5.813 at alpha = 0.001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Do We Compute the Sum of Logarithms of N-gram Probabilities?\n",
        "\n",
        "In n-gram language models, probabilities are often very small, especially for longer sequences. Instead of directly multiplying these probabilities, we compute the **sum of logarithms** of the probabilities. Here's why this approach is used and why it is important.\n",
        "\n",
        "### 1. **Small Probabilities**\n",
        "\n",
        "In typical n-gram models, the probability of a sequence of words is calculated by multiplying the probabilities of individual n-grams. For example, the probability of a sequence like \"the market is rising\" might be computed as:\n",
        "\n",
        "$$\n",
        "P(\\text{\"the\"}) = 0.1, \\quad P(\\text{\"market\" | the}) = 0.05, \\quad \\dots\n",
        "$$\n",
        "\n",
        "Multiplying these small probabilities together yields an extremely small number, which can cause **numerical underflow** (i.e., the result becomes too small to represent in a computer).\n",
        "\n",
        "### 2. **Logarithms for Numerical Stability**\n",
        "\n",
        "To avoid underflow and to handle very small numbers efficiently, we use **logarithms**. Logarithms have the following useful property:\n",
        "\n",
        "$$\n",
        "\\log(P(w_1, w_2, \\dots, w_n)) = \\log(P(w_1)) + \\log(P(w_2 | w_1)) + \\dots + \\log(P(w_n | w_1, \\dots, w_{n-1}))\n",
        "$$\n",
        "\n",
        "By applying the logarithm, we transform the **product of probabilities** into a **sum of logarithms**. This makes it easier to work with small values and avoids the numerical problems associated with multiplying many small numbers.\n",
        "\n",
        "### 3. **Simplification of Calculations**\n",
        "\n",
        "- **Logarithms** simplify the model's calculations by turning a **multiplicative** process into an **additive** one.\n",
        "- The logarithm of any probability is always **negative** (since probabilities are between 0 and 1), but the sum of these logarithms can be handled much more easily.\n",
        "\n",
        "This transformation allows us to compute the **log-likelihood** of a sequence efficiently. For example, the log-likelihood of a sequence $( w_1, w_2, \\dots, w_n )$ is given by:\n",
        "\n",
        "$$\n",
        "\\text{log-likelihood} = \\sum_{i=1}^{n} \\log P(w_i | w_{i-1}, \\dots, w_1)\n",
        "$$\n",
        "\n",
        "Maximizing the log-likelihood is equivalent to maximizing the likelihood but is much easier to compute.\n",
        "\n",
        "### 4. **Log-Likelihood Maximization**\n",
        "\n",
        "Maximizing the likelihood of a sequence is essential in language modeling, and using logarithms makes this process more computationally feasible. The sum of logarithms allows for easier **optimization**, particularly when using **maximum likelihood estimation (MLE)**.\n",
        "\n",
        "### 5. **Explanation of Kneser-Ney Smoothing**\n",
        "- **Kneser-Ney smoothing** is a technique used to adjust the probabilities of n-grams, especially for those that have never been seen in the training corpus. It reduces the impact of unseen n-grams by distributing probability mass to less frequent n-grams, thus making the model more robust to rare events.\n",
        "- The function first calculates the raw frequency of n-grams, then applies the smoothing technique by modifying the counts of n-grams based on their lower-order counterparts.\n",
        "\n",
        "These functions help in building a language model by counting the frequency of n-grams and applying smoothing to handle unseen combinations of words."
      ],
      "metadata": {
        "id": "ssFI1Y_DPsVs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_nO9ujja6Iw8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}